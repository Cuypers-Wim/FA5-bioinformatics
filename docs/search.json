[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bioinformatics for Malaria Molecular Surveillance",
    "section": "",
    "text": "About\nWelcome to the Bioinformatics Module of the Fighting Malaria Across Borders (FiMAB) international training programme, created by the Institute of Tropical Medicine (ITM) in Antwerp (Belgium) and supported by VLIR-UOS. Its primary goal is to support the implementation of targeted sequencing assays (in particular, AmpliSeq) to strengthen malaria molecular surveillance and help guide national control programmes. In conjunction with laboratory training, this bioinformatics course is intended to allow young academics around the globe to become familiar with molecular surveillance as a key activity to monitor transmission, sources of epidemics and the emergence and spread of drug resistance mutations in the Plasmodium parasite.\nDevelopment of this course was supported by VLIR-UOS and the Institute of Tropical Medicine.\nThis work is licensed under CC BY-NC-SA 4.0, which means that you are free to copy, redistribute or adapt any of its contents, provided that you do reference us (see citation information below) and the original license, do indicate if modifications were made, do distribute those contributions under the same license and do not use the material for commercial purposes."
  },
  {
    "objectID": "index.html#scope-of-the-course",
    "href": "index.html#scope-of-the-course",
    "title": "Introduction to Bioinformatics for Malaria Molecular Surveillance",
    "section": "Scope of the course",
    "text": "Scope of the course\nThis course aims to provide an overview of key bioinformatics aspects related to performing molecular surveillance research in Plasmodium. It is divided into the following sections:\n\nIntroduction to the Unix shell (command line interface)\nIntroduction to R\nPopulation genetics and molecular epidemiology for Plasmodium [to follow later]\n\nSection 1-2 are online self-paced modules, whereas section 3 will include classroom lectures and practical sessions. Evaluation exercises will be conducted on the ITM course page."
  },
  {
    "objectID": "content/intro.html#what-is-bioinformatics-anyway",
    "href": "content/intro.html#what-is-bioinformatics-anyway",
    "title": "Introduction",
    "section": "What is bioinformatics anyway?",
    "text": "What is bioinformatics anyway?\nBioinformatics is a scientific discipline that develops and utilises computational methods to analyse large amounts of biological data, typically molecular sequence data such as DNA, RNA and amino acid sequences, as well as annotations describing those sequences. It spans the entire spectrum of collecting, storing and annotating data, to modelling, predicting and discovering new biological insights, from the level of individual molecules, to cells, organisms and populations. Bioinformatics is a highly interdisciplinary field that brings together many different types of researchers, including biologists, computer scientists, statisticians, clinicians and even chemists and physicists. A closely related term that you might see being used interchangeably with bioinformatics, is computational biology. However, there is no consensus on a clear distinction between the two1; like so many things in nature (e.g. the concept of a species), scientific disciplines are often hard to define and delineate.1 \nhttps://biology.stackexchange.com/questions/3192/is-computational-biology-different-from-bioinformatics\nhttps://www.nature.com/subjects/computational-biology-and-bioinformatics\nhttps://www.ebi.ac.uk/training/online/courses/bioinformatics-terrified/what-bioinformatics/\nhttps://www.genome.gov/genetics-glossary/Bioinformatics\n\n\nA few examples of popular topics in bioinformatics are:\n\nPopulation genetics and molecular epidemiology (what we will be focusing on in this course)\nAnalysis of gene/protein expression and regulation, e.g. in disease models (for RNA-seq spatial and single-cell approaches are become more prevalent)\nStructural bioinformatics, e.g. predicting the structure of proteins\nNetwork and systems biology: mapping and analysing the relationships between interacting biomolecules such as proteins, metabolites and their signal cascades, e.g. gene regulatory networks)\nComparative genomics and phylogenetics: studying the ancestry of species, genes or entire genomes through time and space\nGenomic annotation and cataloguing genetic mutations and associated diseases in databases"
  },
  {
    "objectID": "content/intro.html#computational-thinking",
    "href": "content/intro.html#computational-thinking",
    "title": "Introduction",
    "section": "Computational thinking",
    "text": "Computational thinking\nWe hope that this course can teach you a few computational thinking and problem solving skills that will help you along your bioinformatics journey. The learning curve in computational biology can be quite steep at times and the path is littered with arcane commands and obtuse syntax, but as you practice the concepts introduced in this course on your own, your command-line efficiency will improve and you will start to spot similarities across different types of environments and languages. Through this course, we hope to arm you with the necessary skills to make tasks like running custom analysis scripts or installing bioinformatics software seem a little less daunting."
  },
  {
    "objectID": "content/unix/1-unix-intro.html#learning-objectives",
    "href": "content/unix/1-unix-intro.html#learning-objectives",
    "title": "1  What is a CLI?",
    "section": "1.1 Learning objectives",
    "text": "1.1 Learning objectives\n\nKnowledge of what a Unix shell and the CLI are and why/when they can be useful.\nSetting up your own Unix environment.\nFamiliarity with basic bash commands for e.g., navigation, moving/copying and creating/deleting/modifying files and directories.\nIntroduction of a few more advanced commands and concepts like redirection, piping and loops.\nFirst look at scripts and how they can be used in the context of DNA sequencing pipelines for variant calling."
  },
  {
    "objectID": "content/unix/1-unix-intro.html#resources",
    "href": "content/unix/1-unix-intro.html#resources",
    "title": "1  What is a CLI?",
    "section": "Resources",
    "text": "Resources\nThis section of the course draws inspiration from the following resources:\n\nConor Meehan’s UNIX shell tutorial (CC BY-NC-SA 4.0)\nMike Lee’s Unix Crash Course (https://doi.org/10.21105/jose.00053)\nData Carpentry’s Introduction to the Command Line for Genomics (https://doi.org/10.5281/zenodo.3260560 CC-BY 4.0)\nRonan Harrington’s Bioinformatics Notebook (MIT)\nA Primer for Computational Biology by Shawn T. O’Neil (CC BY-NC-SA)\nEric C. Anderson’s Bioinformatics Handbook (Chapter 4) (MIT)\nCourse on UNIX and Genomic Data"
  },
  {
    "objectID": "content/unix/1-unix-intro.html#what-is-unix",
    "href": "content/unix/1-unix-intro.html#what-is-unix",
    "title": "1  What is a CLI?",
    "section": "1.2 What is Unix?",
    "text": "1.2 What is Unix?\nUnix is a family of operating systems, with one of their defining features being the Unix shell, which is both a command line interface and scripting language.\nIn simpler terms, shells look like what you see in the figure below and they are used to talk to computers using a CLI - i.e., through written text commands - instead of via a graphical user interface (GUI) where you primarily use a mouse cursor.\n\n\n\nBash shell in WSL\n\n\nThere exist many different flavours of Unix, collectively termed “Unix-like”, but the ones you will most likely encounter yourself are Linux (which itself comes in many different varieties we call distributions, e.g. Debian, Ubuntu, Fedora, Arch, etc.) and MacOS. These operating systems come with a built-in Unix shell. While Windows also comes with a command line interface (Command Prompt and PowerShell), it is not a Unix shell and thus uses different syntax and commands. We’ll dig into how you can get your hands on a Unix shell on a Windows machine in a later section. The most ubiquitous Unix shell is Bash, which comes as the default on most Linux distributions."
  },
  {
    "objectID": "content/unix/1-unix-intro.html#why-bother-learning-the-unix-shell-as-a-bioinformatician",
    "href": "content/unix/1-unix-intro.html#why-bother-learning-the-unix-shell-as-a-bioinformatician",
    "title": "1  What is a CLI?",
    "section": "1.3 Why bother learning the Unix shell as a bioinformatician?",
    "text": "1.3 Why bother learning the Unix shell as a bioinformatician?\nEven if you are primarily a wet lab scientist, learning the basics of working with CLIs offers a number of advantages:\n\nAutomation: CLIs and scripting excel at performing repetitive tasks, saving not only time, but also lowering the risk of mistakes. Have you ever tried manually renaming hundreds of files? Or adding an extra column to an Excel spreadsheet with millions of rows?\nReproducibility: reproducibility is key in science and by using scripts (and other tools like git, package managers and workflow systems) you can ensure that your analyses can be repeated more readily. This is in stark contrast to the point-and-click nature of GUIs.\nBuilt-in tools: the Unix shell offers a plethora of tools for manipulating and inspecting large (text) files, which we often deal with in bioinformatics. E.g., DNA sequences are often stored as plain text files.\nAvailability of software: many bioinformatics tools are exclusively built for Unix-like environments.\nAccess to remote servers: Unix shells (usually bash) are the native language of most remote servers, High Performance Computing (HPC) clusters and cloud compute systems.\nProgrammatic access: CLIs and scripts allows you to interact in various ways (e.g., via APIs) with data that is stored in large on-line databases, like those hosted by NCBI or EBI.\n\nAs a concrete example of what we will be using the shell for, consider the task of processing hundreds of Plasmodium DNA sequencing reads with the goal of determining the genetic variation in these samples (e.g., the presence of SNPs). Suppose we were to do this in a GUI program, where we would open each individual sample and subject it to a number of analyses steps. Even if each step were to only require a few seconds (in reality, minutes or even hours…), this would take quite a long time and be prone to errors (and quite boring!). With shell scripting, we can automate these repetitive steps and run the analysis without requiring human input at every step. Some of the key techniques we will use for this are:\n\nNavigating to directories and moving files around.\nLooping over a set of files, calling a piece of software on each of them.\nExtracting information from a particular location in a text file.\nCompressing and extracting files.\nChaining commands: passing the output of one tool to another one. E.g., after aligning reads to a reference genome, the resulting output can be fed to the next step of the pipeline, the variant caller.\nEtc."
  },
  {
    "objectID": "content/unix/1-unix-intro.html#dont-get-discouraged",
    "href": "content/unix/1-unix-intro.html#dont-get-discouraged",
    "title": "1  What is a CLI?",
    "section": "1.4 Don’t get discouraged",
    "text": "1.4 Don’t get discouraged\nLearning to use the shell, or learning programming languages and bioinformatics skills in general, can be daunting if you have had little experience with these types of tasks in the past. Don’t worry though, just take your time and things will become easier over time as you gain more experience.\nWe do not expect you to be able to memorize every single command and all of its option. Instead, it is more important to be aware of the existence of commands to perform particular tasks, and to be able to independently retrieve information on how to use them when the need arises.\nFinally, the appendix (Section A.1) of the course also contains a bunch of tips and tricks to keep in mind while learning your way around the shell."
  },
  {
    "objectID": "content/unix/1-unix-intro.html#a-note-on-terminology",
    "href": "content/unix/1-unix-intro.html#a-note-on-terminology",
    "title": "1  What is a CLI?",
    "section": "1.5 A note on terminology",
    "text": "1.5 A note on terminology\nYou will often see the terms command line (interface), terminal, shell, bash, unix (or unix-like) being thrown around more or less interchangeably (including in this course). Most of the time, it is not terribly important to know all the minute differences between them, but you can find an overview here if you are curious: https://astrobiomike.github.io/unix/unix-intro."
  },
  {
    "objectID": "content/unix/2-unix-setup.html#online-unix-environment",
    "href": "content/unix/2-unix-setup.html#online-unix-environment",
    "title": "2  Setting up your own Unix shell",
    "section": "2.1 Online Unix environment",
    "text": "2.1 Online Unix environment\nWe have provided two different options for getting access to an online unix environment: through Binder (free, but less powerful) or through GitHub Codespaces (free for 60 hours per month).\nBoth options will launch an environment containing all relevant training files, based on this GitHub repository.\n\n2.1.1 GitHub Codespaces\nTo access Codespaces, you will first need to create a GitHub account via https://github.com/signup. Just follow the instructions and be sure to enable one of the two-factor authentication options (via a TOTP app like Authy, Google Authenticator or Microsoft Authenticator, or via text messages), otherwise you might not receive access to Codespaces.\nAfterwards, you can click this link, optionally change the region to the one closest to you under change options (but leave the machine type on 2-core to remain eligble for 60 free hours!), and then press the Create codespace button.\n\n\n\nCreating a new codespace\n\n\n\n\n\nLaunching a new codespace\n\n\nSetting up the codespace can take a while, but eventually you will be greeted by a VSCode environment. The terminal (a bash shell) is accessible at the bottom (or by pressing the hamburger icon in the top left and selecting “new terminal”). This is where you will be able to run the Unix commands introduced in the next chapters.\n\n\n\nTerminal inside VSCode editor in GitHub Codespace\n\n\nYou will only receive 60 hours of free usage of Codespaces per month. This means you should manually shutdown your codespace whenever you are done with it. Otherwise it will keep running for 30 more minutes (by default). Just closing your browser will not shut down the workspace. Instead, you need to manually shut it down from within the codespace (by clicking the &gt;&lt; button in the bottom left corner and selecting stop current codespace) or by browsing to https://github.com/codespaces and shutting it down from that page.\n\n\n\n\n\n\nWhat are GitHub and git?\n\n\n\nGitHub is a place to host code and software via a tool named git, which is a version control system. It allows you to keep track of the history of your code, easily revert changes and allows for collaborating with multiple people on the same project. We will not go into further detail on using version control, but for now just remember that it can play an important role in scientific reproducibility.\nIf you want to learn more about git already, you can have a look at the following resources:\n\nhttps://happygitwithr.com/\nhttps://hwheeler01.github.io/CompBio/github/\nhttps://pmoris.github.io/git-workshop/ (self-promotion)\n\n\n\n\n\n\n\n\n\nWhat is GitHub CodeSpaces\n\n\n\nSimilar to Binder, Codespaces are development environments that are hosted in the cloud. This is a paid service provided by GitHub/Microsoft, which offers 60 hours of free usage per individual per month. Instead of Jupyter notebooks, Codespaces use code editors, like VSCode and Jetbrains IDEs, which come bundled with a bash terminal too.\nYou can find more info in the GitHub Codespaces docs.\n\n\n\n\n2.1.2 Binder\nAs an alternative to CodeSpaces, we have also created a Binder environment. It will operate similar to CodeSpaces and provide you with an online environment containing all of the required files as well as a Unix shell (bash).\n\n\n\n\n\n\nWhat is Binder?\n\n\n\nBinder is a service that allows people to share a customized compute environment based on a Git repository. It is mainly aimed at sharing Jupyter Notebooks (Python), but it also supports RStudio, Shiny and fortunately for us, a plain bash terminal too.\nYou can find more info on the Binder website.\n\n\nTo access the remote bash shell on Binder, browse to https://mybinder.org/v2/gh/pmoris/FiMAB-bioinformatics/HEAD and wait for the launcher to start. This process can take quite a while, so be patient.\n\n\n\nLaunching the Binder environment\n\n\nEventually, you should be greeted by a screen (Jupyter Lab) with a number of launchers. Simply select the one labelled “Terminal” (a black square with a white $) and you should be all set.\n\n\n\nStarting a new Bash shell"
  },
  {
    "objectID": "content/unix/2-unix-setup.html#local-unix-environment",
    "href": "content/unix/2-unix-setup.html#local-unix-environment",
    "title": "2  Setting up your own Unix shell",
    "section": "2.2 Local Unix environment",
    "text": "2.2 Local Unix environment\nIf you are using MacOS or Linux, then you will already have access to a Unix shell (either bash or zsh, which will mostly behave identical for our purposes). To access it, simply search for a program called Terminal (or search for anything resembling “command”, “prompt” or “shell”).\nIn case you are using a Windows machine, things are slightly more complex and different methods exist, each with their own pros and cons. You could use a fully-fledged virtual machine like VirtualBox to emulate a Linux machine within Windows. Or you could rely on the minimal bash emulator that comes bundled with git for windows. However, nowadays we recommend that you use the Windows Subsystem for Linux (WSL), which was developed by Microsoft itself. In our opinion, it is one of the most polished methods to get access to a (nearly) full-featured Linux environment from within Windows, without the overhead of a full virtual machine or dual boot setup (dual boot means you install two different operating systems on your machine, and you switch between them when booting). For instructions on how to set it up, you can refer to this section.\n\n2.2.1 Download the course files\nRegardless of what type of local Unix environment you use, you will need to download the files that we will be using in our examples and exercises. You can do this directly on the command line or by manually downloading the files in the correct location.\n\nOpen your terminal and cd to a location where you want to place the training files.\nEnter the command git clone https://github.com/pmoris/FiMAB-bioinformatics.git.\nAfterwards, a new directory named FiMAB-bioinformatics will have been created.\n\nIt should look similar to this:\n$ git clone https://github.com/pmoris/FiMAB-bioinformatics.git\nCloning into 'FiMAB-bioinformatics'...\nremote: Enumerating objects: 7, done.\nremote: Counting objects: 100% (7/7), done.\nremote: Compressing objects: 100% (7/7), done.\nremote: Total 7 (delta 0), reused 7 (delta 0), pack-reused 0\nReceiving objects: 100% (7/7), done.\nAlternatively,\n\nBrowse to https://codeload.github.com/pmoris/FiMAB-bioinformatics/zip/refs/heads/main\nSave the .zip file in a directory accessible by your Unix environment. For Windows/WSL, the easiest option is to choose the Linux file system (e.g., \\\\wsl.localhost\\Ubuntu\\home\\pmoris), which is accessible by clicking the Linux/WSL entry in your explorer.\nExtract/unzip the file.\n\n\n\n\nLinux file system inside Windows File Explorer\n\n\n\n\n2.2.2 WSL installation\nIf you are using an updated version of Windows 10 (or 11), you should meet all the requirements and can simply follow the installation instructions listed here: https://learn.microsoft.com/en-us/windows/wsl/install. We recommend that you follow the instructions for WSL 2 (default), rather than the older WSL 1, and use the default Ubuntu 22.04 distribution (Linux comes in many different flavours, Ubuntu being one of the more popular ones).\nBriefly:\n\nOpen Windows PowerShell as administrator by right clicking your Windows Start Menu or searching for it in your list of applications.\nType wsl --install and press enter.\nAfterwards, restart your PC.\nYou can then launch WSL by searching for wsl or Ubuntu in your start menu.\nThe first time you launch WSL, you will need to configure it.\n\nIf you use software like RStudio or VSCode, you can tell these programs to use WSL as their built-in terminal from now on, instead of Command Prompt.\n\n\n\n\n\n\nWSL1 vs WSL2\n\n\n\nWSL 2 is the newer version of WSL 1. For most tasks, WSL 2 tends to be much faster, hence why we (and Microsoft) recommend using it in favour of the previous version. However, WSL 2 is only faster when you interact with files that are stored directly on the WSL file system, rather than working directly on the Windows file system. More info on the distinction between these file systems can be found further below and in Microsoft’s WSL documentation.\nYou can switch between WSL1 and WSL2 on the fly by just calling wsl --set-version &lt;distro_name&gt; 2 (or 1) in PowerShell, so feel free to experiment for yourself.\nFor a full overview of the differences, check out: https://docs.microsoft.com/en-us/windows/wsl/compare-versions.\n\n\n\n2.2.2.1 Configuring WSL\nMicrosoft also provides an excellent tutorial on setting up your WLS environment, which you can find here.\n\nAfter installing WSL and a Linux distribution, you will have access to it via its own built-in terminal emulator. It should be located in your Windows Start Menu with a name corresponding to the distribution that you installed, e.g. Ubuntu 20.04 LTS, or simply wsl.\nThe first time you run WSL, you will need to setup a Linux username and password. Note that while you are entering a password, nothing will appear on the screen, but this is intended (blind typing). The username will determine, among other things, the name of your home folder, whereas the password will grant you administrator rights (referred to as super users or admins in Linux land; the sudo command is used to invoke these rights, see Section A.3).\nYou will also need to upgrade the packages by running the following command: sudo apt update && sudo apt upgrade, followed by your password.\nFor more information, check the docs.\n\n\n2.2.2.2 Accessing files across the Windows and WSL file systems\n\n\n\n\n\n\nNote\n\n\n\nSome of the information below might be a bit confusing at this point, but things should become more clear after working your way through the Unix section of this course.\n\n\nNewer versions of WSL will automatically add a shortcut to the WSL file system in your Windows File Explorer (look for Tux, Linux’ penguin mascot). The file path will look similar to \\\\wsl$\\Ubuntu\\home\\&lt;user name&gt;\\Project, indicating that Windows treats the WSL file system as a sort of network drive. You can also open a file location in Windows File Explorer from within a WSL terminal (e.g. after you browse to a particular directory cd ~/my-project) by simply using the command explorer.exe . (don’t forget the dot!).\nVice versa, you can also access the Windows file system from within WSL because it is mounted under /mnt/c. So, you could for example do something like cp /mnt/c/Users/&lt;user name&gt;/Downloads/file-downloaded-via-webbrowser ~/projects/filename.\nMore information can be found in the WSL documentation.\n\n\n2.2.2.3 Windows Terminal\nEven though WSL comes with its own terminal application, it is rather bare-bones and can make some operations like copying and pasting via CTRL+C/CTRL+V a bit tricky (you will need to use CTRL+SHIFT+C to copy and right mouse click to paste). Fortunately, Microsoft has also been working on a new terminal emulator that is much nicer to work with. Meet the Windows Terminal."
  },
  {
    "objectID": "content/unix/3-unix-enter-the-shell.html#interacting-with-the-shell",
    "href": "content/unix/3-unix-enter-the-shell.html#interacting-with-the-shell",
    "title": "3  Using the shell",
    "section": "3.1 Interacting with the shell",
    "text": "3.1 Interacting with the shell\nWhen you launch your (Bash) shell, you will be greeted by what is called a shell prompt: a short snippet of text followed by a cursor, which indicates that the shell is waiting for input. The prompt can look different on different systems, but it often consists of your linux username followed by the name of your machine (like in the picture below) or sometimes just a single $ symbol. When you see the prompt, you can enter commands interactively and execute them by pressing enter.\n\n\n\nA bash shell prompt waiting for user input\n\n\nAlready note that you cannot use your mouse cursor to move around your terminal. You will need to use your arrow keys (or shortcuts) to move around while typing commands."
  },
  {
    "objectID": "content/unix/3-unix-enter-the-shell.html#command-syntax",
    "href": "content/unix/3-unix-enter-the-shell.html#command-syntax",
    "title": "3  Using the shell",
    "section": "3.2 Command syntax",
    "text": "3.2 Command syntax\nUnix commands generally follow the format:\ncommand [OPTIONS] argument\nwhere,\n\ncommand is the name of the (usually built-in) command that you want to execute.\n[OPTIONS] is a list of optional flags to modify the behaviour of the command. They are often preceded by a single (-) or double (--) dash.\nargument is a thing that your command can use. E.g., it can be a file name, a short piece of text (or string)\n\nTry it yourself with the following command:\necho “Hello world!”\n\n\n\n\n\n\nWhat did that do? (Click me to expand!)\n\n\n\n\n\necho is a command that simply prints a message to your screen (technically, to the standard output stream (stdout) of the terminal). echo is the command, teling the shell what we want to do. \"Hello world!\" is the target, in this case the message we want to print.\nWe place the message between quotes (\") because it contains spaces, and as you will see, spaces (and certain other special characters) can cause confusions. For now, just note that the message that gets printed, is whatever was written between the quotes, but not the quotes themselves.\n$ echo \"Hello world!\"\nHello world!\n$"
  },
  {
    "objectID": "content/unix/3-unix-enter-the-shell.html#tips-and-hints",
    "href": "content/unix/3-unix-enter-the-shell.html#tips-and-hints",
    "title": "3  Using the shell",
    "section": "3.3 Tips and hints",
    "text": "3.3 Tips and hints\nWe have compiled a number of helpful tips in the appendix of this course (Section A.1), some of which will hopefully be helpful on your journey towards mastering the unix shell. For now, we recommend at the very least checking out the section on tab-completion and your command history. In fact, you can give it a try already. Just press the up arrow and see if you can recall your previous command! Next, try to type out ec, and press &lt;tab&gt;, to see auto-complete in action."
  },
  {
    "objectID": "content/unix/4-unix-navigation.html#layout-of-the-unix-file-system",
    "href": "content/unix/4-unix-navigation.html#layout-of-the-unix-file-system",
    "title": "4  Navigating the Unix file system",
    "section": "4.1 Layout of the Unix file system",
    "text": "4.1 Layout of the Unix file system\nAll files and directories (or folders) in Unix are stored in a hierarchical tree-like structure, similar to what you might be used to on Windows or Mac (cf. File Explorer). The base or foundation of the directory layout in Unix is the root (/) (like the root of a tree). All other files and directories are built on top of this root location. When navigating the file system, it is also important to be aware of your current location. This is called the working directory.\nThe address of a particular file or directory is provided by its filepath: this is a sequence of location names separated by a forward slash (/), like /home/user1. Note that this differs from the convention in Windows, where backslashes (\\) are used in file paths instead.\nThere are two types of file paths: absolute and relative paths.\n\nAbsolute file path: this is the exact location of a file and is always built up from the root location. E.g., /home/user1/projects/document.txt.\nRelative file path: this is the relative address of a file compared to some other path. E.g., from the perspective of /home/user1, the file document.txt is located in projects/document.txt.\n\n\n\n\nOverview of the Unix file system or directory layout\n\n\n\n4.1.1 Home sweet home: ~\nAnother important location is the home directory. In general, every user has their own home directory, found in /home/username. A frequently used shortcut for this is the tilde symbol (~). Depending on the current user, this will refer to a particular directory under /home/..\n\n\n\n\n\n\nHow can user1 write the file path to document.txt using the ~ shortcut?\n\n\n\n\n\n~/projects/document.txt\n\n\n\n\n\n4.1.2 Where am I? . shortcuts\nThe dot (.) also has an important function in file paths:\n\n. represents the directory you are currently in, i.e. the working directory.\n\nE.g., while inside the projects directory, any files inside can be accessed using either filename or ./filename.\n\n.. represents the parent directory of the working directory.\n\nE.g., from /home/user1/Desktop, the relative path to file document.txt can be written as ../projects/document.txt.\nThese expressions can be nested; while inside the projects directory, ../../user2 can be used to access the user2 home directory."
  },
  {
    "objectID": "content/unix/4-unix-navigation.html#moving-around-the-file-system",
    "href": "content/unix/4-unix-navigation.html#moving-around-the-file-system",
    "title": "4  Navigating the Unix file system",
    "section": "4.2 Moving around the file system",
    "text": "4.2 Moving around the file system\nIn this section we will introduce a few essential commands that allow you to navigate the file system: pwd, cd and ls.\n\n4.2.1 pwd: avoid getting lost\npwd stands for print working directory and it does exactly that: it allows you to figure out where you are in the file system. For example, in the figure above, user1 would generally find themselves in their home directory upon login:\n$ pwd\n/home/user1\n\n\n4.2.2 cd: on the move\nNext, there is the cd command. This is used to move between directories (the name derives from change directory). Simply follow the command name by a file path to navigate there: cd &lt;filepath&gt;. To move from user1’s home directory to the projects directory:\ncd projects\nNote that you can use the special symbols we saw earlier as navigational shortcuts:\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\ncd ~\nChange to home directory (/home/username)\n\n\ncd ..\nChange to parent directory (e.g., go up 1 directory)\n\n\ncd /\nChange to the root location\n\n\n\n\n\n4.2.3 ls: show me what you got\nFinally, we have the ls command. Its name stands for listing and it will list the names of the files and directories in the current working directory. The basic structure of the command ls [OPTIONS] &lt;target&gt;, with &lt;target&gt; being an optional path to a directory.\nTo continue upon our previous example, from inside /home/user1/projects we would see:\n$ ls\nDRX333466_1.fastq.gz    DRX333466_2.fastq.gz    document.txt\nNote that we did not specify a path, in which case ls will just list the contents of the current working directory. If we do specify a path, we will of course be shown the contents of that particular location:\n$ ls /home\nuser1   user2\nBy default, the files and directories are listed in alphabetically order and depending on your terminal settings, files and directories might even be colour-coded differently.\nls also comes with a few handy optional flags to modify its behaviour:\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\nls -l\nShow detailed list view\n\n\nls -hl\nShow detailed list view and print file sizes in a human readable format\n\n\nls -a\nList all files and directories, including hidden ones\n\n\nls -lha\nCombine all options into one command\n\n\nls --help\nShow more information on the ls command and its options\n\n\nls\n\n\n\n\n\n\n\n\n\n\nWhat are hidden files?\n\n\n\nEarlier, we mentioned that . is used to refer to the current working directory, but it actually has a second function as well. Any file or directory name that starts with a dot (like /home/user1/.ssh) will be hidden and not displayed by default when using ls, hence the need for the -a flag.\nLinux often hides system or configuration files to avoid cluttering up your (home) directory. We will not deal with hidden files directly in this course, but one of the situations where you might encounter them are when modifying your .bashrc file (e.g., when creating custom functions, aliases or tweaking your PATH Section A.4) or when managing SSH keys for remote server access Section A.6).\n\n\nThe ls -l command is particularly useful, because it shows all types of additional information.\n\n\n\n\n\n\nWhat do the different columns in the output of ls -l represent?\n\n\n\n\n\n$ ls -l\ntotal 83764\n-rw-r--r-- 1 pmoris pmoris 14367565 Dec  7 09:39 3B207-2_S92_L001_R1_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 16622378 Dec  7 09:39 3B207-2_S92_L001_R2_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 13592342 Dec  7 09:39 MRA1242_S28_L001_R1_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 15821981 Dec  7 09:39 MRA1242_S28_L001_R2_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 12131772 Dec  7 09:39 NK6_S57_L001_R1_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 13226198 Dec  7 09:39 NK6_S57_L001_R2_001.fastq.gz\nThe first column represents the permissions of the files/folders. In a nutshell, these determine things like who can read or write (= modify, including deletion) particular files. There is a column for the owner, a group of users and everyone else. There is more info in the appendix (Section A.5). The next column showing a 1 for each entry, you can ignore for now (they represent hard links, a concept we will not dive into). The two names in the following columns are the user and the group owner of the file. Next is the size of the file in bytes. If we had used the -h flag, the size would have been shown in KB, MB or GB instead. Next we have the time of the last modification and finally the name of the file/directory."
  },
  {
    "objectID": "content/unix/4-unix-navigation.html#exercises",
    "href": "content/unix/4-unix-navigation.html#exercises",
    "title": "4  Navigating the Unix file system",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\n\nNavigate to your home directory and list all the files and folders there. Try typing the path with and without using the ~. Rely on tab-completion to assist you and avoid typos (Section A.1).\nPrint the name of the current working directory to your screen.\nList the contents of the ./training/data/fastq/ directory of the course files, without first moving there. Experiment with absolute and relative paths.\nWhat is the most recent modification date of the file Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa found in the ./training/unix-demo/ directory?\nTry to search for the file penguins.csv: what is the absolute path to it on your machine?\nNavigate to ./training/data/fastq/ to make it your working directory (double check using pwd!). What is the relative path to the penguins.csv file from here?\nSuppose your working directory is still ./training/data/fastq/. What will the result of pwd be after running each of the following commands in succession?\n\n\ncd ../\ncd ../unix-demo/\ncd files_to_loop_through/../../data/..\ncd /\ncd ~"
  },
  {
    "objectID": "content/unix/4-unix-navigation.html#summary",
    "href": "content/unix/4-unix-navigation.html#summary",
    "title": "4  Navigating the Unix file system",
    "section": "4.4 Summary",
    "text": "4.4 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nAbsolute versus relative file paths\nRoot (/) and home directory (~)\n. represents the current working directory\n.. represents the parent directory\npwd: print the path of the current working directory\ncd &lt;path&gt;: navigate to the given directory\nls &lt;path&gt;: list files and directories in the given location\nHidden files contain a . at the start of their name and are not visible by default"
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#examining-files",
    "href": "content/unix/5-unix-files-and-dirs.html#examining-files",
    "title": "5  Working with files and directories",
    "section": "5.1 Examining files",
    "text": "5.1 Examining files\n\n5.1.1 cat: viewing short files\nThe most basic command for viewing a file is the cat &lt;file&gt; command. It simply prints all of the contents of a file to the screen (= standard output).\n$ cd training/unix-demo\n$ cat short.txt\nOn the Origin of Species\n\nBY MEANS OF NATURAL SELECTION,\n\nOR THE PRESERVATION OF FAVOURED RACES IN THE STRUGGLE FOR LIFE.\n\nBy Charles Darwin, M.A., F.R.S.,\n\nAuthor of “The Descent of Man,” etc., etc.\n\nSixth London Edition, with all Additions and Corrections.\n\n\n\n\n\n\nTry using cat on the file named long.txt and see what happens (Click me to expand!)\n\n\n\n\n\nThe entire file (in this case, the entirety of the Origin of Species by Charles Darwin) is printed to the screen. This works, but is not very easy to navigate. Especially if you consider the fact that this text is still just tiny compared to some of the files that we deal with in bioinformatics; it is only ~0.03% of the size of the (rather short) human Y chromosome (~60 Mbp) that we will look at next.\n\n\n\nWhile cat is very useful, it is clearly not suitable for large text files. Since long files are very prevalent - and not only in bioinformatics - we need an alternative. Enter the less command.\n\n\n5.1.2 less: viewing large files\nThis tool is suitable for streaming very large files which would otherwise crash a normal text editor or program like Excel. less will open the contents of the file in a dedicated viewer, i.e. your terminal and prompt will be replaced by a unique interface for the less tool. You can exit this interface by pressing q.\nUsing less, we can have a look at the (truncated) version of the human Y chromosome (in FASTA format):\n$ less Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa\n\n\n\nOpening a FASTA file in less\n\n\n\n\n\n\n\n\nNavigating inside less\n\n\n\n\n\n\nUse arrow keys to navigate. space and b can also be used to go forward and backwards, and page up/page down work as well.\nPress g to jump to the start of the file\nPress G (shift + g) to jump to the end of the file\nType / followed by a string to search forward (?[string] for backwards search) and n/N for the previous/next match\nTo exit, press Q\nUse the help command for more info: less --help\n\n\n\n\n\n5.1.2.1 FASTA file format\n\n\n\n\n\n\nDNA sequence file formats: FASTA\n\n\n\nThe FASTA file format (usually denoted by a .fa or .fasta file extension) is very common in bioinformatics. As you can see, the FASTA files contain a long stretch of nucleotides, which in our case represent the sequence of the human Y chromosome (or at least the first ~6,000,000 basepairs). The sequence itself is usually broken up over multiple lines. At the very top of the file there is a header or identifier, which always starts with the &gt; symbol, followed by a short description. FASTA files can store one or multiple sequences, each with their own header.\nFASTA files are a type of text-based or plain text files, meaning that we can simply read them using a tool like cat or less. This seems obvious, but we will later encounter another file type, namely binary files, where this is not the case.\nFASTA files are commonly used in genomics to store the reference genome of the organism we are studying. A reference genome can be used as a template to which we map (or align) new DNA sequence reads we have generated.\n&gt;Pf3D7_01_v3 | organism=Plasmodium_falciparum_3D7 | version=2020-09-01 | length=640851 | SO=chromosome\nTGAACCCTAAAACCTAAACCCTAAACCCTAAACCCTGAACCCTAAACCCTGAACCCTAAA\nCCCTAAACCCTGAACCCTAAACCCTAAACCCTGAACCCTAAACCCTGAAACCTAAACCCT\nGAACCCTAAACCCTGAACCCTGAACCCTAACCCTAAACCCTAAACCTAAAACCCTGAACC\nCTAAACCCTGAACCCTGAACCCTAAACCCTGAACCCTAAACCCTAAACCCTGAACCCTAA\nACCCTGAACCCTAAACCCTAAACCCTGAACCCTGAACCCTAAAACCTAAACCCTAAACCC\nTAAACCCTAAACCCTGAACCTAAACCTAAAACCTAAAACCTAAAACCCTGAACCCTTACT\nTTTCATTTCTTCTTCTTATCTTCTTACTTTTCATTCTTTACTCTTACTTACTTAGTCTTA\nCTTACTTACTCTTACTTACTTACTCTTATCTTCTTACTTTTCATTTCTTAGTCTTACTTA\n...\n\n\n\n\n\n5.1.3 head and tail: viewing the start or end of files\nSometimes we are not interested in viewing the entire file, but just the first few or last lines. The commands head and tail were created for exactly this use case. The basic usage is simply head &lt;filepath&gt;, but there again are a few optional flags that can alter the default behaviour.\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\nhead file\nPrint the first 10 lines of a file\n\n\ntail file\nPrint the last 10 lines of a file\n\n\nhead -n # file\nDisplay the first # lines of a file\n\n\ntail -n # file\nDisplay the final # lines of a file\n\n\ntail -n +2 file\nDisplay all lines except for the first one (i.e., perform tail, but start at line 2)\n\n\n\nLet us inspect the first lines of one of the (uncompressed) FASTQ files in the unix-demo directory:\n$ head PF0512_S47_L001_R1_001.fastq\n@M05795:43:000000000-CFLMP:1:1101:16134:1717 1:N:0:47\nTTGGTCAAGATCTTTTACATTCCATGCACACAAAAGAATTCTTCTACTTGTCTGATCCTTTTTCATTATATTTATTATCTTTTTTTATTTTTCCTCTCCTTTATTTTCATAATTATCATACATATTTTTATATTCATCACCTAAATGTCTCCATTTAAAACCATAAATAGTTCCTAATTCGTTAACTTCTCTATGACATAATTTTCTATTATCTCTTTCTCTTATCCACATCTCCGAGCCCACGAGACGTCGCACCCTCTCCTATGCCCTCTTCTGCTTGAAACACAAACGCCACCACCT\n+\n-8B&lt;CGGFGGGDGGGGGEFGGGAFFEFCFGGDC888C,CF@@FFG,6FC,C,,,,&lt;&lt;,&lt;&lt;CCE,,&lt;C,,,&lt;6B,;B,&lt;,&lt;A@EEC+,:@,,94,,5549@?F,@@?D&lt;,C,,9@,A;,C,9,A,:A=,&gt;,9,9@;,,,,94,,,,9,9499=,9&gt;,,,,,9+,9,,,9,,,493@,,,99+60++6==93=C+C++++6++4&lt;=;=D+?=+42+43+33*?=;+**5*1*)0*108))18=):))))0)185)))1))--:)1*/*16)0/(01//8***()**((((((,(((/((,(,\n@M05795:43:000000000-CFLMP:1:1101:20605:1731 1:N:0:47\nGAAAAAGGAAGAGAATTGAACTTTTGGCAGCAAACTCAAACATTATAAGTGAAATTAAGATGCCCAAGTCTGTGCTCAATCTCATTTTTTGTTTTTGTGTTTTTCCTTCAATCTCTTCATGTATTCAGTTATTTTTAAT\n+\n-@CCCGGGGGGGGCGFG@&lt;EFGGGGAFFFEGGC8FEF9,,;,CE,C,,,&lt;,,,,&lt;C,,,,&lt;,,6;,,,;;C,&lt;,6;C,,&lt;,,:,:@,5,8+,,9:@+:,,9,9,99AE?,,,959AA?,9,,,994,,9=,9&gt;ED,,,9\n@M05795:43:000000000-CFLMP:1:1101:9135:1768 1:N:0:47\nCGTTAAAATCTTGCTCCTCATCACTACTAACCTTTTGTTCATTCTCATCACAAATATTATCCTTATCTTCATTATCTACTTCATCTACATTATTTTTTAT\n\n5.1.3.1 FASTQ file format\n\n\n\n\n\n\nDNA sequence file formats: FASTQ\n\n\n\nAside from FASTA files, another typical DNA sequence format is FASTQ (extension .fastq or .fq), which is used to store the raw output of high-throughput sequencing (like AmpliSeq) in the form of short read fragments. Like FASTA, it is text-based format, but instead of just identifiers and sequences, it also contains quality scores associated with each nucleotide. Each read is described by four lines of text. A single read might look like this:\n@SEQ_ID\nACTACTAGGATTGAGGACGTCCTCCCAACAGGGAGTTGGTTGGGCGCCCGGTGCCGTCATGTCCGATCGCTATCTACGTCTAGTACTAGAGAATTATACA\n+\n\"H85&lt;EI4A533D;E1A56C@@GHI=BFGIIH6;F=3::HGF8C;9/&gt;;EI?E4I(F?FID&lt;CBAFFD69E:BB&gt;+#&lt;58H:/&lt;&gt;IE;881&'D':F&lt;&lt;H(\"\n\n\n\n\n\n\n\nLine\nDescription\n\n\n\n\n1\nidentifier: always starts with ‘@’ and contains information about the read (e.g., instrument, lane, multiplex tag, coordinates, etc.\n\n\n2\nThe sequence of nucleotides making up the read\n\n\n3\nAlways begins with a ‘+’ and sometimes repeats the identifier\n\n\n4\nContains a string of ASCII characters that represent the quality score for each base (i.e., it has the exact same length as line 2)\n\n\n\nFASTQ files often come in pairs, which are usually named the same with a slightly different suffix (e.g., sample_1_R1.fastq and sample_1_R2.fastq). These pairs are reads of the same fragment in the opposite direction. In a nutshell, paired-end sequencing is used because the additional information provided by reads being paired can help with mapping repetitive regions of the genome.\n\n\n\n\n\n\n\n\nTry inspecting the contents of one of the .fastq.gz files in the training/data/fastq directory (Click me to expand!)\n\n\n\n\n\nThe less command most likely behaves as we expect it to, but if we were to try cat, head or tail, you would see a lot of gibberish being printed to your screen.\nThe reason is that these FASTQ files are compressed using gzip (which is why the file extension ends in .gz). Because of this, they are no longer plain text files, but compressed binary versions. We will learn more about compressed files and how to deal with them in a later section of this course. In a nutshell though, compressed files either need to be unpacked or they require a tool that was designed to handle them (e.g., zcat, zgrep, zless. In most linux distributions, zless is called automatically when you try to less a file with the .gz extension, which is why the text seemed normal.\n\n\n\n\n\n\n5.1.4 wc: counting lines\nSometimes we’re not interested in the specific contents of a file, but only in how long it is in terms of text (not file size). For this we can use the wc command: it can count the number of lines, words and characters in a text file. By default, it prints all of this information, but by providing the -l flag, you can tell the command to only return the number of lines. Taking the example of our FASTQ file again, we see:\n# number of lines, words and characters\n$ wc PF0512_S47_L001_R1_001.fastq\n582940   728675 69598721 PF0512_S47_L001_R1_001.fastq\n\n# number of lines only\n$ wc -l PF0512_S47_L001_R1_001.fastq\n582940 PF0512_S47_L001_R1_001.fastq\n\n\n\n\n\n\nHow many reads are there in this FASTQ file? (Click me to expand!)\n\n\n\n\n\nEach read in a FASTQ file consists of four lines (see Section 5.1.3.1). Therefore, we can simply divide the output of wc -l by four to figure out the number of reads. In this case:  {582,940 \\over 4} = 145,735 reads."
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#editing-files",
    "href": "content/unix/5-unix-files-and-dirs.html#editing-files",
    "title": "5  Working with files and directories",
    "section": "5.2 Editing files",
    "text": "5.2 Editing files\nYou can edit files directly on the command line, i.e. without opening them in a text editor like Notepad(++) or VSCode, by using the nano command. This can come in quite handy in a variety of situations, like fixing small errors in your code before running it or to editing configuration files. Similar to less, nano will open a special editor interface where you can edit text files.\n\n\n\nThe nano text editor\n\n\n\n\n\n\n\n\nNavigating inside nano\n\n\n\n\nYour mouse pointer won’t work. Use arrow keys to move instead.\nTo save, press ctrl+o, followed by return/enter.\nTo exit, press ctrl+x, followed by return/enter.\n\n\n\nThere exist many other editors, one of the most beloved, yet notorious ones, being vim. It is quite a bit more powerful, but also more complex. Even closing vim has become somewhat of a meme because it can be difficult to figure out (it’s &lt;escape&gt; followed by :q and `enter/return``)."
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#moving-things-around",
    "href": "content/unix/5-unix-files-and-dirs.html#moving-things-around",
    "title": "5  Working with files and directories",
    "section": "5.3 Moving things around",
    "text": "5.3 Moving things around\nNow that we have spent some time on inspecting files, let us move on to moving them around.\n\n5.3.1 cp: copying files and directories\ncp stands for copy and it does exactly what it says on the tin. It can copy files, as well as directories to a new location. For files, the syntax is as follows:\ncp path/to/source_file path/to/destination\nWhere source is the original file that you want to copy and destination is the new path where you want to place the copy. If the destination is a directory, the file will be placed inside of it with the same name as the original file. If the destination does not exist yet, it will be used as the new name for the copy.\nWhen we want to move around directories instead of files, we need to add the -r flag (short for --recursive).\ncp -r path/to/source_directory path/to/destination_file\nYou can even copy multiple files at the same time!\n$ cp file_1 file_2 file_3 /path/to/destination\n\n$ ls /path/to/destination\nfile_1 file_2 file_3\n\n\n5.3.2 Intermezzo: globbing and wildcards\nNow seems like a good time to introduce the concept of the globbing and wildcards. Globbing allows you to perform operations on multiple files. By providing specific patterns, the shell will be able to expand them into a list of matching file names. The patterns are built using wildcards, one of the most common ones being the asterisk *.\nHow does this work? Well, * can represent any number of other characters. For example, the string *.txt can match all file names ending with .txt in your directory. Let’s look at a concrete example, using the ls command we saw earlier:\n$ ls\nHomo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa  PF0512_S47_L001_R1_001.fastq  files_to_copy  files_to_delete  files_to_move  long.txt  short.txt  penguins.csv\n\n$ ls *.txt\nlong.txt  short.txt\nAs you can see, we can make ls list only those files that match a particular pattern, instead of showing all the files in the directory. What happens behind the scenes is that *.txt is expanded to long.txt short.txt. This means that the command that the shell eventually sees is actually ls long.txt short.txt.\nSimilarly, we can combine wildcards with the new cp command.\n$ cp *.txt ..\n\n$ ls ..\n\n\n\n\n\n\nWhat do you think this previous command will do? (Click me to expand!)\n\n\n\n\n\n*.txt will be expanded to a list of all .txt files in the current working directory. The cp command will then try to copy each of those files to the destination, which is .. in this case. As we saw before, .. represents the parent directory of the current directory (see Section A.2).\nThis means that the command is equivalent to cp long.txt short.txt /absolute/path/to/parent_directory and will move all the .txt files in the current directory to its parent directory.\n\n\n\nAnother type of wildcard is [...]. This is used to supply a list of possible character matches. For example, the glob pattern [bcr]at would match bat, cat and rat.\nThere are a number of other wildcards, but even * alone will prove to be very useful. If you’d like to find out more, have a look at this resource. Also note that globbing looks similar to regular expressions, but while related, these two concepts behave slightly differently. We will not dive into regular expressions here though, but we will mention them again when we talk about the search tool grep.\nTo summarise, globbing is an extremely powerful tool that will allow you to more easily target multiple files. We will rely on the power of globbing a lot going forward.\n\n\n5.3.3 mv Moving or renaming files and directories\nThe mv (move) command behaves very similar to the cp command, the main difference being that the former allows you to move rather than copy files and directories. Also note that mv is used to rename files as well.\n# move around/rename a particular file\nmv &lt;source_file&gt; &lt;destination_file&gt;\n\n# move a directory\nmv &lt;path/to/source_directory&gt; &lt;path/to/destination_directory&gt;"
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#creation-and-destruction",
    "href": "content/unix/5-unix-files-and-dirs.html#creation-and-destruction",
    "title": "5  Working with files and directories",
    "section": "5.4 Creation and destruction",
    "text": "5.4 Creation and destruction\nWe will end this section by teaching you how to create and delete files or directories.\n\n5.4.1 Creating files\nThere are several ways of creating new files in Unix, but one of them is the nano command that we already introduced earlier. If you provide a file name that does not yet exist, nano will create the file for you.\n$ ls\n\n$ nano new_file.txt\n# inside nano, use ctrl+x to save the file and then close the editor via ctrl+x\n\n$ ls\nnew_file.txt\nAnother option is to use the touch /path/to/file command. This will just create a new empty file at the specified location.\n\n\n5.4.2 mkdir: creating directories\nmkdir stands for make directory and it does just that:\n$ mkdir new_dir\n\n$ ls\nnew_dir\nOne useful optional flag is -p/--parents: this allows you to create multiple nested (parent) directories in one go. For example, if we’re inside an empty directory, we could call:\nmkdir -p my/new/multi/level/directory\nAnd all the intermediate directories would be automatically created.\n\n\n5.4.3 rm: removing things\n\n\n\n\n\n\nWatch out…\n\n\n\nBe careful while learning your way around the command-line. The Unix shell will do exactly what you tell it to, often without hesitation or asking for confirmation. This means that you might accidentally move, overwrite or delete files without intending to do so. For example, when creating, copying or moving files, they can overwrite existing ones if you give them the same name. Similarly, when a file is deleted, it will be removed completely, without first passing by a recycle bin.\nNo matter how much experience you have, it is a good idea to remain cautious when performing these types of operations.\nFor the purposes of learning, if you are using your own device instead of a cloud environment, we recommend that you work in a dedicated playground directory or even create a new user profile to be extra safe. And like always, backups of your important files are invaluable regardless of what you are doing.\n\n\nThe rm command (remove) is used to delete files and directories. Be warned though, once deleted, things are really gone. There is no recycle bin or trash folder where you can restore deleted items!\n# for files:\nrm &lt;file path&gt;\n\n# for directories\nrm -r &lt;directory path&gt;\nFor files, this works as expected, but for directories you need to provide the -r flag (or --recursive). This tells Unix to remove the directory recursively, i.e. all of its contents need to be removed as well. If you don’t use this option, you will see the following warning:\nrm directory\nrm: cannot remove 'test/': Is a directory\n\n\n\n\n\n\nProtected files\n\n\n\n\n\nSometimes, files will be protected and you will get another warning message when you try to remove them. If you are really sure that you want to delete them, you can type y and press enter. Alternatively, you can cancel the operation (by entering n or by pressing ctrl+c) and try again, but this time providing the -f/--force option.\n# create a new empty file\n$ touch protected-file\n# change its permissions so that it is protected against writing and deleting (see appendix for more info on file permissions)\n$ chmod a-w protected-file\n# try to remove it\n$ rm protected-file\nrm: remove write-protected regular empty file 'protected-file'? n\n# use the --force flag\n$ rm -f protected-file"
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#exercises",
    "href": "content/unix/5-unix-files-and-dirs.html#exercises",
    "title": "5  Working with files and directories",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nCreate a new directory named “my_dir” inside the ./training/unix-demo directory. Next, without using cd first, create another directory named my_sub_dir inside of it. Finally, again without using cd, create a final directory named my_sub_sub_dir inside of that one.\nRead the last 20 lines of the FASTA file in the ./training/unix-demo directory.\nCreate a new text file named lines inside my_subdir using nano. Store the number of lines of the file long.txt inside. Then read it using cat and less.\nNavigate to the files_to_copy directory and copy its contents to the my_sub_dir directory. What is the relative path of the destination to use?\nMove the file under files_to_move to its parent directory.\nRemove all the files under files_to_delete using a glob pattern.\nRename the directory files_to_delete to empty_dir.\nList the contents of the ./training/unix-demo directory."
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#summary",
    "href": "content/unix/5-unix-files-and-dirs.html#summary",
    "title": "5  Working with files and directories",
    "section": "5.6 Summary",
    "text": "5.6 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nFASTA file format is used to store DNA sequences\nFASTQ file format is used to store sequence reads and their quality scores\nCompressed files (e.g., .gz) are smaller in file size, but are no longer plain text files and require special tools.\nThe permission of files can be set to prevent users from reading, writing or (re)moving them.\n\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\ncat &lt;path/to/file&gt;\nprint the content of files\n\n\nless &lt;path/to/file&gt;\nread the contents of (large) files in a special viewer\n\n\nhead/tail &lt;path/to/file&gt;\nview the first or last lines of a file\n\n\nwc &lt;path/to/file\ndisplay the line/word/character count of a file\n\n\nnano &lt;path/to/file&gt;\nopen a file (or create a new file) in the nano text editor\n\n\ncp [-r] &lt;source&gt; &lt;destination&gt;\ncopy a file/directory to a new location\n\n\nmv [-r] &lt;source&gt; &lt;destination&gt;\nmove a file/directory to a new location (or rename it)\n\n\nrm [-r] &lt;path/to/file_or_directory&gt;\npermanently remove a file/directory\n\n\nmkdir &lt;path/to/directory&gt;\ncreate a new directory"
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#searching-in-files-grep",
    "href": "content/unix/6-unix-more-commands.html#searching-in-files-grep",
    "title": "6  More advanced commands",
    "section": "6.1 Searching in files: grep",
    "text": "6.1 Searching in files: grep\nBeing able to search through (long) text files is incredibly useful in a wide range of scenarios. For this, we make use of the grep command. It is a very powerful and complex command, with many different options to tweak its behaviour, but even just the basic version can already be a lifesaver. The basic syntax is\ngrep \"PATTERN\" &lt;path/to/target_file&gt;\nFor example, we can look for the word “evolution” in the Origin of Species:\n$ grep \"evolution\" long.txt\nevolutionists that mammals are descended from a marsupial form; and if\nAt the present day almost all naturalists admit evolution under some\nevolutionists; but there is no need, as it seems to me, to invoke any\nEveryone who believes in slow and gradual evolution, will of course\nof gradual evolution, through the preservation of a large number of\na strong disbeliever in evolution, but he appears to have been so much\nhistorian will recognise as having produced a revolution in natural\nthe fact would be fatal to the theory of evolution through natural\nthe revolution in our palæontological knowledge effected by the\nopposed to the admission of such prodigious geographical revolutions\nhas thus been arrived at; and the belief in the revolution of the earth\nsubject of evolution, and never once met with any sympathetic\nagreement. It is probable that some did then believe in evolution, but\nevolution. There are, however, some who still think that species have\nwe can dimly foresee that there will be a considerable revolution in\nIt might not be obvious from the above snippet, but inside your own terminal, the matching words in the search results will be highlighted. As you can see, grep will return each line that contains a match. Also note how partial matches like revolution are returned as well.\n\n\n\n\n\n\nTry searching for the string “species” instead. Do you think these are all the hits? (Click me to expand!)\n\n\n\n\n\nWhen you run grep \"species\" long.txt, you will indeed find many occurrences of this word. However, we are missing all the occurrences of “Species”. Try running grep \"Species\" long.txt to compare the results. Lastly, try running the command grep -i \"species\" long.txt. This option will\nRemember, capitalization matters to grep (and to Unix as a whole)!\n\n\n\nAside from the -i option explained in the box above, there exist several other flags to improve your search results.\n\n\n\n\n\n\n\nOption\nEffect\n\n\n\n\n-i\nCase insensitive (i.e., ACTG = actg)\n\n\n-n\nAlso print the line number of the result\n\n\n-c\nCount the number of matches\n\n\n-5\nPrint 5 lines surrounding each match\n\n\n-e/-E/-P\nUse regular or extended regular expressions\n\n\n-r\nRecursive search through all files in a folder\n\n\n\nThe -# option is particularly useful to learn more about the context around you search result. You can supply any number of lines here, which will get printed both before and after each match. The example below, you can see how it helps us find the identifier of a DNA read that contains a particular sequence (AACCGGGGT):\n$ grep -3 AACCGGGGT PF0512_S47_L001_R1_001.fastq\n+\nCCCCCGGCFGGGGFGGGGDGCEFGGGGGGGGGGGGGGGGGGGGGGFGGGGGGGGFFGGGFGGGGGGGGGGGGGGFGGGGGGGGGGGGGGGGGGGGGGGGGD+,A=FG,:&lt;?F5FGGGD&gt;F9BDBDE9FEDAFGAFGGB=EDF,,&gt;@FCCF;@;D;CF;,5;DEGG84,@@,3@ED&lt;@,,@7,;@,7EC2=E&gt;C977=\n@M05795:43:000000000-CFLMP:1:2106:16840:21815 1:N:0:47\nAGCCATACCAAGACCACAATTCTGAAGAGGAAACAAAACAAAAAAAAAAAAATAATTAAAAAAAAAAAAATTTAAAATTAAAAAAAAAATTTTTTTATTAAAATAATAAATATTAATTTTTATAATATAAATAAAATCCTATTTTACCCCACCAACCGGGGTTCATCCCCGGGCTCTTATACACATTTCCTAACCCACAAAAAGTACGAACAACACGCAACCCCCCTTCTGCCTTAAAAAAAAAACAAATCAAAATACACAAATATATCGAACATACAGCAACTACAAATGAAGATGTGGT\n+\nCCCCCGGFGGGFGGGGCFG9@@C&lt;FGDG8EGGGFGGGGGGGGGGGGGGGGGD,:C,96CFD,8&gt;FG7FGG,,,,B&lt;B9,9CFFGGCG+3,,8BF@@,=8,8,=,C,,3@,,,3,3@,DGGFC,,,,,,7,,,,7&gt;C,,3,,,7@@@:9,6**5,,**4*1*=8,,+5&gt;=:****3/+&gt;2;;92++2+++4++29*/:**9*1**3*0*/*/95)1*1))))29*05)202.:96*/-@=&lt;(7:(.)00()))))))).-,)(())--)-(((.().,.:)8..(,).).))-4))..)))(\n@M05795:43:000000000-CFLMP:1:2106:14940:21820 1:N:0:47\nThe -c options makes grep return the total number of matches it found. This method of counting is useful to complement the wc command, in case you are presented with a file that does not have such an orderly structure as the FASTQ format we saw earlier. To demonstrate, consider the penguins.csv file, which contains morphological data on three different penguin species.1 We can count the number of Adelie penguin records via:1 Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. doi:10.1371/journal.pone.0090081\ngrep -c \"Adelie\" penguins.csv\n152\nIn some situations we want to search through multiple files simultaneously. This is where the -r/--recursive flag comes in. It allows us to target a directory and search through all of its contents (including subdirectories). Let us try searching for the same DNA sequence as before, but this time targeting all the files in the unix-demo directory:\n$ grep -r \"AACCGGGGT\" .\n./PF0512_S47_L001_R1_001.fastq:AGCCATACCAAGACCACAATTCTGAAGAGGAAACAAAACAAAAAAAAAAAAATAATTAAAAAAAAAAAAATTTAAAATTAAAAAAAAAATTTTTTTATTAAAATAATAAATATTAATTTTTATAATATAAATAAAATCCTATTTTACCCCACCAACCGGGGTTCATCCCCGGGCTCTTATACACATTTCCTAACCCACAAAAAGTACGAACAACACGCAACCCCCCTTCTGCCTTAAAAAAAAAACAAATCAAAATACACAAATATATCGAACATACAGCAACTACAAATGAAGATGTGGT\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:AATAAAACCGGGGTTGATACCACCACTTCCAGGTTCCCACATTCCAAGTCCCCTCAGCCA\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:CTGGAGTCAGGACGTGAGCCGACTTGCTTAAAAATAAATCCACATGGCTGAACCGGGGTT\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:ACAGCTAACCGGGGTTTTAGTATATGTGCCACATCTCTGTAAATGTTCACTTCCTAGGCA\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:TATGATCGTGCCACTGCACTTCAACCGGGGTGACAAAGCGAAAACCGTGTCTCTAAAAAA\n\n\n\n\n\n\nHow would you search through .txt files only? (Click me to expand!)\n\n\n\n\n\nInstead of using the -r flag, we can also rely on globbing again (see Section 5.3.2). To search for the string “needle” in all .txt files in a particular folder, we can do the following:\ngrep \"needle\" path/to/directory_with_txt_files/*.txt\n\n\n\nWe already mentioned regular expressions in the previous section: they allow you to search for particular patterns that can match more than one exact string of text. This is tremendously useful, but we will not dive deeply into how they work during this course. If you are interested, you can check out an excellent tutorial here.\nOne special pattern that we will introduce is the start or end of line anchor. You can search for patterns that start at the beginning of a line by prefixing the pattern with a ^ symbol. Similarly, you can search for patterns that occur at the end of a line by using a $ symbol. For example, if we search for grep \"&gt;@\" read.fastq in a FASTQ file, we will only retrieve lines that start with the @ symbol, ignoring any @ symbols that are used elsewhere on a line.\n\n\n\n\n\n\nThe above command could still return matches that are not read headers. Why? (Click me to expand!)\n\n\n\n\n\nAs we saw in Section 5.1.3.1, every read in a FASTQ file is represented by four lines, the first of which is the header and which always starts with an @ symbol. So far so good, but the fourth line, which contains the quality score of each base in the sequence, can also start with an @ symbol, because @ is just another score value that could happen to be occur for the first base in the sequence.\n\n\n\nWe will see more elaborate use cases for grep when we introduce the Unix concepts of piping and redirection."
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#tabular-data-cut",
    "href": "content/unix/6-unix-more-commands.html#tabular-data-cut",
    "title": "6  More advanced commands",
    "section": "6.2 Tabular data: cut",
    "text": "6.2 Tabular data: cut\nWe already encountered tabular data (the penguin dataset in .csv format) when talking about grep. Tabular data files like .csv are a very common format, and not just in bioinformatics.\n\n\n\n\n\n\nTabular data and .csv files\n\n\n\nTabular data files are usually plain text files, where each row corresponds to a record (e.g., an individual penguin), and each column represents a particular field (e.g., species, flipper length, body mass, etc.). The columns can be separated by different field delimiters or separators. In .csv files, these are usually commas (comma separated values), but they can also be TABS (.tsv) or semicolons (;).\n\n\nA particularly useful Unix tool for manipulating tabular data files, is cut. It allows us to extract particular columns from these files. The syntax is as follows:\ncut [OPTIONS] target_file\n\n\n\n\n\n\n\nOption\nEffect\n\n\n\n\n-d \",\"/--delimiter \";\"\nChange the default delimiter (TAB) to another character like ,\n\n\n-f 1\nSelect the first column\n\n\n-f 2,3\nSelect the second and third column\n\n\n-f 1-3,6\nSelect columns one through three and columns six\n\n\n--complement -f 1\nSelect all columns except for the first one\n\n\n-r\nRecursive search through all files in a folder\n\n\n\n\n6.2.1 SAM file format\nAside from .csv and .tsv files, there are many bioinformatics file formats that also follow a tabular lay-out. One of these is the SAM file format.\n\n\n\n\n\n\nSAM: Sequence Alignment/Map Format\n\n\n\nThe SAM file format is a tab-delimited text file that stores information about the alignment of sequence reads to a reference genome.\nWhen considering the steps that are taken during the variant calling analysis of sequence reads, SAM files result when the reads inside a FASTQ file are processed by an alignment tool like bwa and minimap. These tools take each individual read corresponding to a particular sample and try to map them to their most likely position in a reference genome.\nSAM files consists of an optional header section followed by an alignment section.\nThe header lines always start with an @ symbol and contain information about e.g., the reference genome that was used and the sorting of the alignments. This section is not yet tab-delimited.\nThe alignment section contains a tab-delimited line for each sequence read that was aligned to the reference. There are 11 mandatory fields, containing information on the position of the read in the reference genome (i.e., where it was mapped), the sequence itself, its quality score ( = the quality of each base pair of the sequence during sequence calling, cf. FASTQ format), its mapping score ( = how well the read aligned to the reference genome), etc.\n\n\n\nCol\nField\nType\nBrief description\n\n\n\n\n1\nQNAME\nString\nQuery template NAME\n\n\n2\nFLAG\nInt\nbitwise FLAG\n\n\n3\nRNAME\nString\nReferences sequence NAME\n\n\n4\nPOS\nInt\n1- based leftmost mapping POSition\n\n\n5\nMAPQ\nInt\nMAPping Quality\n\n\n6\nCIGAR\nString\nCIGAR string\n\n\n7\nRNEXT\nString\nRef. name of the mate/next read\n\n\n8\nPNEXT\nInt\nPosition of the mate/next read\n\n\n9\nTLEN\nInt\nobserved Template LENgth\n\n\n10\nSEQ\nString\nsegment SEQuence\n\n\n11\nQUAL\nString\nASCII of Phred-scaled base QUALity+33\n\n\n\nWe will not dive into the details of each of these fields for now, but if you are interested you can check out the official documentation or this useful write-up.\nA typical SAM file will look something like this:\n@SQ     SN:ref  LN:45\n@SQ     SN:ref2 LN:40\nr001    163     ref     7       30      8M4I4M1D3M      =       37      39      TTAGATAAAGAGGATACTG     *       XX:B:S,12561,2,20,112\nr002    0       ref     9       30      1S2I6M1P1I1P1I4M2I      *       0       0       AAAAGATAAGGGATAAA       *\nr003    0       ref     9       30      5H6M    *       0       0       AGCTAA  *\nr004    0       ref     16      30      6M14N1I5M       *       0       0       ATAGCTCTCAGC    *\nr003    16      ref     29      30      6H5M    *       0       0       TAGGC   *\nr001    83      ref     37      30      9M      =       7       -39     CAGCGCCAT       *\nx1      0       ref2    1       30      20M     *       0       0       aggttttataaaacaaataa    ????????????????????\nx2      0       ref2    2       30      21M     *       0       0       ggttttataaaacaaataatt   ?????????????????????\nx3      0       ref2    6       30      9M4I13M *       0       0       ttataaaacAAATaattaagtctaca      ??????????????????????????\nx4      0       ref2    10      30      25M     *       0       0       CaaaTaattaagtctacagagcaac       ?????????????????????????\nx5      0       ref2    12      30      24M     *       0       0       aaTaattaagtctacagagcaact        ????????????????????????\nx6      0       ref2    14      30      23M     *       0       0       Taattaagtctacagagcaacta ???????????????????????\nThis example has two header lines denoting the reference genome contigs and their lengths, followed by 12 aligned reads.\nNow inspect the SAM file in ./training/unix-demo/PF0302_S20.sort.sam and try to identify the different sections that make up the file."
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#file-sizes-du",
    "href": "content/unix/6-unix-more-commands.html#file-sizes-du",
    "title": "6  More advanced commands",
    "section": "6.3 File sizes: du",
    "text": "6.3 File sizes: du\nWe already saw that the ls -lh can be used to figure out the file size of files in a particular directory. du is another tool to do this, but it operates on individual files or directories directly. Like ls, it also provides the -h/--human-readable option to return file sizes in KB/MB/GB, so it is generally recommended to always use this option. When used on a file, it will simply return its size, but when used on a directory, it will output information for all files, as well as the total file size of the entire directory (the final line of the output).\n# targetting an individual file\n$ du -h Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa\n5.9M    Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa\n\n# targetting a directory\n$ du -h training/data/\n284M    training/data/fastq\n62M     training/data/reference\n346M    training/data/"
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#compressed-files-gzip",
    "href": "content/unix/6-unix-more-commands.html#compressed-files-gzip",
    "title": "6  More advanced commands",
    "section": "6.4 Compressed files: gzip",
    "text": "6.4 Compressed files: gzip\nWe already introduced the concept of file compression when talking about the FASTQ files in the training/data/fastq directory. As a reminder, compressed files are binary files (as opposed to human-readable plain text files) that are used to reduce the file size for more efficient storage. Many of the files that we use in bioinformatics tend to be compressed. Some of the tools we use, will not work on compressed files (e.g., try to cat a compressed file and see what happens), so we either need to 1) use specialized tools that expect compressed files as their input, or 2) decompress or extract the files first.\nFor gzipped (.gz) files specifically, we can do this via the gzip and gunzip commands. The former allows us to create a gzip-compressed version of the file, whereas the latter will extract one back to a plain text file. The basic syntax is gzip/gunzip &lt;path/to/file&gt;, but a very useful option is the -k/--keep flag. Without it, compressing a file would replace the uncompressed file with the new compressed one (vice versa: extracting would replace the compressed version with the extracted one), but when using the flag both files will be retained.\n\n\n\n\n\n\nTry compressing the FASTQ file in the unix-demo directory. By how much does its file size change? (Click me to expand!)\n\n\n\n\n\n$ du -h PF0512_S47_L001_R1_001.fastq\n67M     PF0512_S47_L001_R1_001.fastq\n$ gzip --keep PF0512_S47_L001_R1_001.fastq\n$ du -h PF0512_S47_L001_R1_001.fastq.gz\n17M     PF0512_S47_L001_R1_001.fastq.gz\nAfter compressing the FASTQ file with gzip, it shrunk to less than a third of its original size.\n\n\n\nDo note that there exist other types of file compression besides gzip, like .zip/.7zip. In unix we also often make use of tar (which technically is not a compression tool, but a file archiver). File compression and tar can even be combined, leading to files with suffixes like .tar.gz. This allows us to compress entire directories, instead of only individual files.\nTo extract these so called tarballs, we need to use the tar command:\n# extract .tar.gz archive\n$ tar -xzvf tar_archive.tar.gz\ntar_archive/\ntar_archive/3\ntar_archive/2\ntar_archive/1\n\n$ ls tar_archive\n1  2  3\n\n# create .tar.gz archive\n$ tar -czvf new_archive.tar.gz &lt;path/to/target_directory&gt;\nThis command is notorious for how arcane its option flags are, but you can either try to remember it using a mnemonic (“eXtract/Compress Ze Vucking Files”, pronounced like a B-movie vampire) or the meaning of the individual flags (z tells tar that we are using gzip compression, -v stands for --verbose to make the command show more information and output, -c/x switches between compression and extraction mode, -f is the last option and points to the tar file) . And of course, the correct syntax is only a google/tldr search or tar --help call away.\n\n6.4.1 BAM file format\nA final example of a binary file that you might encounter in the bioinformatic analysis of AmpliSeq sequencing is the BAM file format:\n\n\n\n\n\n\nBAM: Binary Alignment Map\n\n\n\nA BAM file is nothing more than a compressed, binary representation of a SAM file. It is used to reduce the file size of SAM files and also improve the speed of specific operations like sorting or retrieving information from the file.\nMany bioinformatics tools can handle BAM file natively. However, similar to gzipped files, BAM files are binary and we can no longer preview them using tools like cat and less.\nOne of the tools that is often used to convert between the two types of alignment file formats is samtools, which is maintained by the same group of people who manage the format specification for SAM/BAM files.\n\n\nAside from BAM, you might also encounter the CRAM file format. This is a more recent and even more highly optimized compressed alternative for sequence alignments, but it is not as commonly used (yet).\nsamtools is a commonly used program to manage SAM/BAM files, which we will introduce later when discussing the structure of variant calling pipelines."
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#downloading-files-wget",
    "href": "content/unix/6-unix-more-commands.html#downloading-files-wget",
    "title": "6  More advanced commands",
    "section": "6.5 Downloading files: wget",
    "text": "6.5 Downloading files: wget\nwget is a command that allows you to download files from a particular web address or URL and place them in your working directory. While there are several optional flags, in its most basic form the syntax is simply: wget URL. This command is not only useful when automating certain tasks, but also crucial if you ever find yourself in a Unix environment that does not have a GUI at all (e.g., compute clusters or cloud servers).\n\n\n\n\n\n\nTry downloading the Plasmodium falciparum 3D7 reference genome in FASTA format from PlasmoDB and store it in ./training/data/results. (Click me to expand!)\n\n\n\n\n\n\nAt the top of the page, click on Data -&gt; Download data files.\nSearch for falciparum 3D7 and then narrow down your search by selecting the most recent release and the FASTA file format. Alternatively, you can click on the Download Archive link in the top and navigate the file directory to the current release.\nThe file name of the 3D7 reference genome in FASTA format is PlasmoDB-66_Pfalciparum3D7_Genome.fasta.\nRight click the file and copy its URL to your clipboard: https://plasmodb.org/common/downloads/release-66/Pfalciparum3D7/fasta/data/PlasmoDB-66_Pfalciparum3D7_Genome.fasta.\nCreate and navigate to the output directory (mkdir -p ./training/data/results and cd ./training/data/results)\nDownload the file here using the command: wget https://plasmodb.org/common/downloads/release-66/Pfalciparum3D7/fasta/data/PlasmoDB-66_Pfalciparum3D7_Genome.fasta.\n\n\n\n\nTwo optional flags that you might find useful are: 1) -o allows you to rename the download file, and 2) -P &lt;path/to/directory saves the file in a directory of your choice instead of the current working directory. Of course, these are just small convenient timesavers, since you can always cd to a particular location and use mv to rename the file afterwards.\nLastly, an alternative to wget that you might encounter at some point is curl. On the whole, it acts quite similar to wget for the most part."
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#retrieving-file-names-basename",
    "href": "content/unix/6-unix-more-commands.html#retrieving-file-names-basename",
    "title": "6  More advanced commands",
    "section": "6.6 Retrieving file names: basename",
    "text": "6.6 Retrieving file names: basename\nbasename is a rather simple command: if you give it a long file path, it will return the final section (i.e., the file name).\n# starting in the `training` directory\n$ pwd\n/home/pmoris/itg/FiMAB-bioinformatics/training\n\n# get the file name for the reference genome we just downloaded\n$ basename data/reference/PlasmoDB-65_Pfalciparum3D7_Genome.fasta\nPlasmoDB-65_Pfalciparum3D7_Genome.fasta\nWe can also use this command to remove a particular suffix from a filename:\n$ basename PF0512_S47_L001_R1_001.fastq .fastq\nPF0512_S47_L001_R1_001\nAt this point in time, it might not seem particularly useful to be able to extract the file name of a file, but when we introduce the concept of for loops and bash scripting, it will become more clear why this can be so useful."
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#sorting-and-removing-duplicates-sort-and-uniq",
    "href": "content/unix/6-unix-more-commands.html#sorting-and-removing-duplicates-sort-and-uniq",
    "title": "6  More advanced commands",
    "section": "6.7 Sorting and removing duplicates: sort and uniq",
    "text": "6.7 Sorting and removing duplicates: sort and uniq\nThe final two commands that we will introduce are yet again tools to manipulate plain text files. The first is sort, which does exactly that you expect it to. It can sort all the rows in a text file. Its syntax is:\nsort [OPTIONS] &lt;./path/to/file&gt;`\nThere are optional flags that allow you to choose the type of order to use (e.g., numerical -n/--numeric-sort instead of alphabetical ), reverse the order of the sort (-r/--reverse) or ignore capitals (-f/--ignore-case).\nThe second command, uniq, is used to remove duplicate lines in a file. It also offers the option to count the frequency of each unique line.\n# given the following file\n$ cat file_with_duplicates.txt\na\na\nb\nb\nb\nb\nc\n\n$ uniq -c file_with_duplicates.txt\n2 a\n4 b\n1 c\nThese two commands are often used in conjunction, because uniq on its own is not capable of filtering out identical lines that are not adjacent. So to truly remove all duplicate lines in a file, we would first need to sort it. In the next section, we will introduce a method of combining these commands in a more convenient way than running them one by one and without needing to create any intermediary files."
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#other-commands",
    "href": "content/unix/6-unix-more-commands.html#other-commands",
    "title": "6  More advanced commands",
    "section": "6.8 Other commands",
    "text": "6.8 Other commands\nOf course, there exist many more Unix commands than the ones we introduced here. We will end this section by briefly mentioning two that you might run into at some point awk and sed. Both of them allow you to search and replace patterns in text files, and with awk you can even perform more complex operations including calculations. We will not dive into them here, but keep them in the back of your mind for the future."
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#exercises",
    "href": "content/unix/6-unix-more-commands.html#exercises",
    "title": "6  More advanced commands",
    "section": "6.9 Exercises",
    "text": "6.9 Exercises\n\nVisit PlasmoDB again and find the download URL for the Plasmodium vivax P01 reference genome sequence in FASTA format. Download it via the command line and store it in ./training/data/reference.\nReport the file size of this reference genome in MBs.\nFind out how many lines of text the file contains.\nSearch through the file for the &gt; character, which is used to denote every chromosome/contig. Use a single command to count them.\nCompress the FASTQ file PF0512_S47_L001_R1_001.fastq in the unix-demo directory using gzip.\nNavigate to the directory ./training/data/fastq and in a single command, extract the forward (PF0097_S43_L001_R1_001.fastq.gz) and reverse (PF0097_S43_L001_R2_001.fastq.gz) of the PF0097_S43 sample, without removing the compressed files. Hint: use globbing!\nSearch both FASTQ files for the read fragment with identifier @M05795:43:000000000-CFLMP:1:1101:21518:5740 2:N:0:43 using a single command.\nCompare the file sizes of the two compressed and uncompressed FASTQ files.\nExtract the columns containing the island and flipper length of each penguin from the ./training/unix-demo/penguins.csv file.\nCount how often the sequence CATCATCATCATCAT occurs in the FASTA file ./training/unix-demo/Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa.\nWhich command can be used to extract the name of the SAM file ./training/unix-demo/PF0302_S20.sort.sam without the .sam suffix?\nWhich reference genome was used to create the SAM file?"
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#summary",
    "href": "content/unix/6-unix-more-commands.html#summary",
    "title": "6  More advanced commands",
    "section": "6.10 Summary",
    "text": "6.10 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nTabular data: .csv, .tsv\nThe SAM/BAM file formats store sequence reads aligned to a reference genome.\n\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\ngrep &lt;pattern&gt; &lt;path/to/file&gt;\nSearch through a (very large) file for the supplied pattern\n\n\ndu &lt;-h&gt; &lt;path/to/file_or_directory&gt;\nCheck how much space a file or directory occupies\n\n\ncut -f [--delimiter \",\"] &lt;path/to/file&gt;\nExtract columns from tabular data using the specified delimiter\n\n\ngzip / gunzip (-keep) &lt;path/to/file&gt;\nCompress or extract a gzip compressed file (.gz)\n\n\nwget &lt;url&gt;\nDownloads a file from the URL to the current directory\n\n\nbasename &lt;path/to/file&gt;\nReturns the name of the file without the path prefix\n\n\nbasename &lt;path/to/file&gt; \nReturns the name of the file and remove the provided suffix"
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#streams",
    "href": "content/unix/7-unix-redirection.html#streams",
    "title": "7  Streams, redirection and piping",
    "section": "7.1 Streams",
    "text": "7.1 Streams\nThe general flow of Unix commands is that we supply a specific input on the terminal, which is supplied to a command, and any output is printed back to the terminal screen. In other words, processes have three different data streams connected to them.\n\n\n\nUnix input and output streams\n\n\nThe output for most commands, like that of echo, cat and ls, is called the standard output or stdout, and it is printed to our terminal screen by default. However, there exists another output stream in Unix, namely the standard error or stderr. This stream will contain error or warning messages produced by commands, and it is also printed to the terminal screen by default. There also exists an input stream, called standard input or stdin, which provides the data that is fed into a program.\nRedirection and piping allows us to make these data streams go to or come from another file or process, instead of the terminal. Connecting these streams in different combination allows us to perform all kinds of useful operations.\n\n7.1.1 Redirecting output\nOne of the most common uses of redirection is redirecting the output to a file. For this, we make use of the greater than operator &gt;:\n$ ls &gt; redirected_output.txt\nsome     files     in   a   directory\n\n$ cat redirected_output.txt\nsome     files     in   a   directory\nIn the example above, the output of ls was not printed to the screen, but redirected to a file named ls_output.txt. Note that if the file does not exist, it will be created for us. However, if the file already exists, it will be overwritten (i.e., its contents will be removed entirely and replaced by our new output).\nA related operator is &gt;&gt;. It will behave similar, with the difference being that &gt;&gt; will instead append its output to existing files, rather than overwriting them.\n$ ls &gt; redirected_output.txt\nsome     files     in   a   directory\n\n$ echo \"A second line!\" &gt;&gt; redirected_output.txt\n\n$ cat redirected_output.txt\nsome     files     in   a   directory\nA second line!\nTechnically, whenever we use redirection, we are targeting a specific stream. Stdout is the default stream, so in the previous examples, &gt; and &gt;&gt; were actually shorthand for 1&gt; and 1&gt;&gt;.\nAs a more concrete example, we can use the redirection operator to store the results of a grep search.\n$ grep \"contig\" ampliseq-variants.vcf &gt; vcf-contigs.txt\n$ cat vcf-contigs.txt\n##contig=&lt;ID=Pf3D7_01_v3,length=640851,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_02_v3,length=947102,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_03_v3,length=1067971,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_04_v3,length=1200490,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_05_v3,length=1343557,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_06_v3,length=1418242,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_07_v3,length=1445207,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_08_v3,length=1472805,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_09_v3,length=1541735,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_10_v3,length=1687656,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_11_v3,length=2038340,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_12_v3,length=2271494,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_13_v3,length=2925236,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_14_v3,length=3291936,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_API_v3,length=34250,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf_M76611,length=5967,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\nIn the example above, we searched for lines containing the word contig in a .vcf file and stored the output in a file called vcf-contigs.txt, instead of just printing the output to the screen. But what is a VCF file anyway?\n\n7.1.1.1 VCF files\n\n\n\n\n\n\nVariant Call Format (VCF)\n\n\n\nVCF is the de facto file format for storing gene sequence variation data. It is a plain text file with tab-delimited columns preceded by header lines with additional metadata (starting with ##), similar to the structure of a BAM file.\nAn example of a VCF file is provided below1:\n\n\n\nDescription of Variant Call Format (VCF)\n\n\nFor now, the most important aspect to remember is that each of the lines in the body of the file store information on the presence of an indel at a particular position in the genome. There are eight mandatory fields for each variant, but the format is flexible and additional fields can be used to store extra information:\n\n\n\n\n\n\n\n\nField\nName\nDescription\n\n\n\n\n1\nCHROM\nThe name of the sequence (typically a chromosome) against which a variant is compared.\n\n\n2\nPOS\nThe reference position.\n\n\n3\nID\nThe identifier of the variant.\n\n\n4\nREF\nThe reference base occurring at this position in the reference sequence.\n\n\n5\nALT\nThe list of alternative alleles found in your samples at this position.\n\n\n6\nQUAL\nA quality score associated with the inference of the given alleles.\n\n\n7\nFILTER\nIndicates which filters the variant has passed. Used as a quality control.\n\n\n8\nINFO\nAdditional info about the variant can be stored here as key-value pairs.\n\n\n\nFor a more in-depth view, we refer to the following excellent resources:\n\nhttps://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format\nhttps://davetang.github.io/learning_vcf_file/\n\nSimilar to SAM/BAM files, there is also a binary version of VCF, namely BCF (Binary variant Call Format). Moreover, like samtools was developed to handle SAM/BAM files, there is dedicated program named bcftools that can be used to work with VCF/BCF files.\n\n\n1 Source: https://vcftools.sourceforge.net/VCF-poster.pdf\n\n\n7.1.2 Redirecting errors\nWithout redirection, most commands print their error and warning messages to the terminal screen. If we use &gt; (or 1&gt;) to redirect the output stream, the stderr will still be printed to the screen (and not be stored in the file that we are redirecting to). To redirect and store the error messages, we need to specify stderr as the stream via 2&lt;.\n# cat on two files normally prints the output of both,\n# but in this case fake_file does not exist\n$ cat real_file fake_file\nfoobar\ncat: fake_file: No such file or directory\n\n# when we redirect stdout to a file\n# stderr is still printed to the screen\n$ cat real_file fake_file &gt; redirected_output.txt\ncat: fake_file: No such file or directory\n\n# when we redirect stderr to a file\n# stdout is still printed to the screen\n$ cat real_file fake_file 2&gt; redirected_errors.txt\nfoobar\nIf we want to redirect both stdout and stderr, we have to use a slightly more complex command:\n# redirect both stdout and stderr to a file\n$ cat real_file fake_file &gt; redirection.txt 2&gt;&1\n\n$ ls redirection.txt\nfoobar\ncat: fake_file: No such file or directory\n\n\n7.1.3 Input redirection\nThe input stream (stdin) can also be redirected. Most commands like cat can open and process a file, but some commands cannot operate directly on a file. Instead, they need to be supplied with data directly. This is where input redirection (&lt;) comes in. We have not yet encountered any commands that need to work on however, so the example below would work equally well without input redirection.\n$ cat &lt; input_file.txt\nlines in\ninput_file.txt\n\n\n7.1.4 Overview of redirection operators\n\n\n\n\n\n\n\nRedirection operator\nResult\n\n\n\n\ncommand &gt; file\nwrite stdout to file, overwriting if file exists\n\n\ncommand &gt;&gt; file\nwrite stdout to file, appending if file exists\n\n\ncommand 2&gt; file\nwrite stderr to file, overwriting if file exists\n\n\ncommand &gt; file 2&gt;&1\nwrite both stdout and stderr to file, overwriting if file exists\n\n\ncommand &lt; file\nread input from file and pass it to command"
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#piping",
    "href": "content/unix/7-unix-redirection.html#piping",
    "title": "7  Streams, redirection and piping",
    "section": "7.2 Piping",
    "text": "7.2 Piping\nPiping allows us to redirect the output of one command, to the input of another command. It is the more common way of redirecting the input stream. Pipes can chain multiple commands one after another so that a complex series of steps can be run in one go, without any intermediary output files. In its simplest form, piping looks like this:\ncommand_1 | command_2\nWhere the first command produces some kind of output that can be used by the second one. For example, we could pipe the output of ls to grep to search through a list of directories and files:\n$ ls\nPF0080_S44_L001_R1_001.fastq.gz  PF0157_S55_L001_R2_001.fastq.gz  PF0329_S56_L001_R1_001.fastq.gz\nPF0080_S44_L001_R2_001.fastq.gz  PF0275_S68_L001_R1_001.fastq.gz  PF0329_S56_L001_R2_001.fastq.gz\nPF0097_S43_L001_R1_001.fastq.gz  PF0275_S68_L001_R2_001.fastq.gz  PF0512_S47_L001_R1_001.fastq.gz\nPF0097_S43_L001_R2_001.fastq.gz  PF0302_S20_L001_R1_001.fastq.gz  PF0512_S47_L001_R2_001.fastq.gz\nPF0157_S55_L001_R1_001.fastq.gz  PF0302_S20_L001_R2_001.fastq.gz\n\n# search through the list of files in the\n# current directory for a particular sample name\n$ ls | grep \"PF0275\"\nPF0275_S68_L001_R1_001.fastq.gz\nPF0275_S68_L001_R2_001.fastq.gz\nNote that in this case the syntax of grep \"pattern\" &lt;file&gt; changes slightly: we only supply the pattern, and the target file is now replaced by the stdout stream of ls. Piping makes it a lot more convenient to manipulate and chain commands in this manner. Doing the same thing without making use of piping takes a lot more work:\n# write the output of ls to a file\n$ ls &gt; ls_output.txt\n\n$ grep \"PF0275\" ls_output.txt\n\n$ rm ls_output.txt\nClearly, the above approach is not very convenient. Especially if you consider the fact that you can chain as many pipes and redirections as you want: command &lt; input.txt | command | command &gt; output.txt. Let’s take a look at a few more examples:\n\n\n\n\n\n\nTry counting the number of grep matches using a pipe instead of the -c flag. (Click me to expand!)\n\n\n\n\n\n# count the number of matches in grep search results\n$ grep \"pattern\" file | wc -l\ngrep will return a single line for each match that it finds. These lines are passed to the stdin of wc -l, which will count the number of lines.\n\n\n\n\n\n\n\n\n\nHow can we remove non-consecutive duplicate lines from a file? (Click me to expand!)\n\n\n\n\n\n# consider a file with non-consecutive duplicate lines\n$ cat file.txt\na\na\nb\nc\nb\n\n# using uniq will only remove the consecutive duplicate line\n$ uniq file.txt\na\nb\nc\nb\n\n# if we first sort and then run uniq, we get the desired output\n$ sort file | uniq\na\nb\nc\nsort will sort the file alphabetically, which results in all duplicates on consecutive lines. If we then pipe this output into uniq, all duplicates will be removed.\n\n\n\nHere is another example of how to use pipes, this time applied to the SAM files that we saw earlier. SAM files sometimes contain a few lines of additional information - called the header and starting with an @ symbol - before the start of the tab-delimited alignments (1 read per line). If we were to use the cut command to extract a particular column from such a file, the first few lines matching the header would cause problems, because these lines do not correspond to the tabular structure of the rest of the file.\n\n\n\n\n\n\nHow can we use a pipe to only apply the cut -f10 command to only the lines after the header? (Click me to expand!)\n\n\n\n\n\n# count the number of lines in the header\n$ grep -c \"@\" alignment-with-header.sam\n2\n\n# use tail -n +3 to print the output of the file starting\n# the 3rd line, then pipe this output into the cut command\n$ tail -n +3 alignment-with-header.sam | cut -f10\nTTAGATAAAGAGGATACTG\nAAAAGATAAGGGATAAA\nAGCTAA\nATAGCTCTCAGC\nTAGGC\nCAGCGCCAT\naggttttataaaacaaataa\nggttttataaaacaaataatt\nttataaaacAAATaattaagtctaca\nCaaaTaattaagtctacagagcaac\naaTaattaagtctacagagcaact\nTaattaagtctacagagcaacta\nWe use the tail command to first extract the parts of the file that we are interested in, and then feed this output into the cut command to select a particular column (in this case, the 10th column corresponds to the sequence).\n\n\n\nAs a final tip on the usages of piping, consider that you can pipe the output of any command to | less. This is extremely convenient whenever the output of a particular command is too long and does not fit on your terminal screen. Of course, in some situations you are probably better of storing the output in a file using a stdout &gt; redirection."
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#further-reading",
    "href": "content/unix/7-unix-redirection.html#further-reading",
    "title": "7  Streams, redirection and piping",
    "section": "7.3 Further reading",
    "text": "7.3 Further reading\nRedirection operations and pipes can be combined in many more complex ways than what we saw here. For example, in case we want to redirect output to both a file and the terminal, we can make use of the tee command, as described here. It is even possible to create more complex nested processes, where you feed the output of multiple different commands into a single command: diff &lt;(ls old) &lt;(ls new); this is called process substitution.\nYou do not need to concern yourself with learning these more advanced concepts for the time being, but just keep in mind that whatever you want to do, the Unix shell likely offers a way of doing it."
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#exercises",
    "href": "content/unix/7-unix-redirection.html#exercises",
    "title": "7  Streams, redirection and piping",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\nSearch for the DNA sequence “aacct” in the truncated human Y chromosome FASTA file and store the output in a file called aacct-hits.txt.\nCount the number of chromosomes in the P. falciparum reference genome fasta file.\nStore the chromosome identifiers of the P. falciparum reference genome fasta file in a file.\nStore the last 40 lines of PF0512_S47_L001_R1_001.fastq in a file named PF0512_S47_L001_R1_001.subset.fastq.\nHow many penguin records are there for each island in penguins.csv? Hint: Try to do it in one go, without grep, by combining multiple pipes (cut, sort and uniq).\nHow can you count the number of unique commands in your command history?\nExtract the header information from ampliseq-variants.vcf, sort it alphabetically, and store it in a file named vcf-header.txt."
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#summary",
    "href": "content/unix/7-unix-redirection.html#summary",
    "title": "7  Streams, redirection and piping",
    "section": "7.5 Summary",
    "text": "7.5 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nVCF files are used to store genetic variant information\nData streams: stdin, stdout and stderr\nRedirecting output to a file to replace (&gt;) or append (&gt;&gt;)\nRedirecting errors to a file (2&gt;)\nPiping the output of one command to the input of another command via |"
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#variables",
    "href": "content/unix/8-unix-variables-loops-scripts.html#variables",
    "title": "8  Variables, loops and scripts",
    "section": "8.1 Variables",
    "text": "8.1 Variables\nVariables are placeholder names to refer to specific values. You can use them as shortcuts to refer back to a specific value or file path. Moreover, they are easy to set and re-use in bash scripts, which we’ll introduce later. To set a variable, we assign a value to a name using the equals sign =. Afterwards, we can always recall the value via the variable’s name, prefixed with a dollar sign $. While not strictly necessary, it is good practice to also enclose the name of the variable in {}, because it makes creating new variable names a lot easier during scripting.\n$ my_value=\"Plasmodium falciparum\"\n\n$ echo ${my_value}\nPlasmodium falciparum\nAs before, we use spaces around the value that we are assigned to the variable. This allows us to use spaces and other special characters inside our value. The variable in our example was a piece of text (a string), but we can also store things like integers (x=101) or booleans (true/false).\nThe main reason we introduced the concept of variables is because they play an important role in (for) loops, so let us move on to that topic now."
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#loops",
    "href": "content/unix/8-unix-variables-loops-scripts.html#loops",
    "title": "8  Variables, loops and scripts",
    "section": "8.2 Loops",
    "text": "8.2 Loops\nLoops provide a powerful method of repeating a set of operations multiple times. They are integral to automation and being able to process large numbers of samples in bioinformatics pipelines, but also very convenient for performing other tasks like renaming a bunch of files. The idea is a bit similar to the concept of globbing, but loops offer a lot more flexibility and control over the process.\nThe most common type of loops are probably for loops:\n$ for nucleotide in A C T G \\\n&gt; do echo ${nucleotide} \\\n&gt; done\nA\nC\nT\nG\nThere are a number of different things going on here:\n\nThe for loop consists out of three different sections:\n\nfor nucleotide in A C T G: this tells bash that we want to start a for loop. It also defines the range of values that our loop will iterate over, in this case the characters A, C, T, and G. Finally, it creates a new variable called nucleotide. During every pass or round of the loop, its value will change to one of the values defined in the loop’s range.\ndo echo $i: this is the body of the loop. It always starts with do and is then followed by one or more commands. In the body, you can make use of the loop variable $nucleotide.\ndone: this notifies bash that the body and loop definition end here.\n\nThis is the first time that we see a multi-line bash command, where we split across new lines using a backslash symbol (\\). We could have just as well written this statement on a single line (for i in a c t g; do echo $i; done), using colons (;) to mark the end of each section of the loop.\n\nInstead of looping over a set of words, we can also loop over a range of values:\n$ for i in {1..3} \\\n&gt; do echo ${i} &gt;&gt; loop.txt \\\n&gt; done\n\n$ cat for_loop.txt\n1\n2\n3\nAlso note that we used a different name for our loop variable this time around; you can use any name you like, but i is a very common placeholder. To make your scripts easier to read, it is good to stick with a reasonable short, but informative name.\n\n\n\n\n\n\nAs a reminder, what would have happened if we had used &gt; instead of &gt;&gt;? (Click me to expand!)\n\n\n\n\n\nThe for loop for i in {1..3}; do echo ${i} &gt; loop.txt; done is basically equivalent to running the following three commands one after another:\necho 1 &gt; loop.txt\necho 2 &gt; loop.txt\necho 3 &gt; loop.txt\nAs we saw in the previous sections, the redirection &gt; will always overwrite the contents of its destination file. So in this case, the file would only contain the final number of the loop, namely 3. Loops always run in the order defined in their range.\n\n\n\nAnother common for loop pattern is the following one, which is used to loop over a set of files. It combines the for loop syntax with glob patterns (Section 5.3.2):\n$ ls ./directory\nsample_1_R1.fastq   sample_1_R2.fastq   text_file.txt\n\n$ for fq in ./directory/*.fastq; do wc -l ${fq}; du -h ${fq}; done\n582940 ./directory/sample_1_R1.fastq\n67M     ./directory/sample_1_R1.fastq\n462334 ./directory/sample_1_R2.fastq\n54M     ./directory/sample_1_R2.fastq\nThere are a few important things to note here:\n\nThe glob pattern ./directory/*.fastq will be expanded by the shell to a list of all files ending with .fastq. Consequently, the for loop will only iterate of the FASTQ files and the .txt file is ignored.\nInside the execution statement of the loop, the current file is referred to via the variable ${fq}.\nWe used a semicolon (;) to write the loop statement on a single line.\nUnlike the previous example, the body of this for loop contains multiple commands: first the filename is printed to the screen using echo, then the number of lines in the file is printed to the screen.\n\n\nLoops, combined with scripting, are incredibly useful when performing more advanced operations, like performing the bioinformatics analysis of DNA sequencing reads. For example, to process the DNA reads generated by an AmpliSeq assay and identify the genetic variants (variant calling analysis), the following steps would be performed by looping over the FASTQ file corresponding to each sample:\nfor every FASTQ file:\n    1. Perform a quality control step\n    2. Map the reads to the reference genome\n    3. Call the variants in the alignment\n\nThere actually exists another type of loop, namely the while loop. These behave similar, but instead of going through a list of items or a range of numbers, the loop will continue for as long as a certain condition is met. You can find more information here in case you are interested."
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#shell-scripts",
    "href": "content/unix/8-unix-variables-loops-scripts.html#shell-scripts",
    "title": "8  Variables, loops and scripts",
    "section": "8.3 Shell scripts",
    "text": "8.3 Shell scripts\nAll the topics that we have covered so far, were performed interactively on the command line. However, we can also write scripts that contain a series of commands, loops and variables, which can be executed in one go. That way, you can queue up a bunch of long-running processes and don’t need to stick around to start up each next step in the process. Moreover, it allows you to reuse the same set of operations in the future. Scripts can even be written in such a way that they can be called using different options, similar to how we can provide different optional arguments to bash commands.\n\n8.3.1 Creating a bash script\nShell scripts are nothing more than executable text files written in a specific scripting language, in our case bash. We can write bash scripts in any type of text editor (like Notepad or VS Code), but we can also do it directly on the command line, by making use of an editor like nano or vim.\nThe only requirements for bash scripts is that the first line contains a shebang directive, like #!/usr/bin/env bash. When the script is executed, this line tells your machine to run the script using the bash interpreter (i.e., that it is a bash script and needs to be treated accordingly). By convention, shell scripts are saved with the .sh extension.\n\n\n8.3.2 Running scripts\nA very simple script might look a bit like this:\n#!/usr/bin/env bash\n\necho \"My first script!\"\nInside a script, you can use any valid bash statement that would work on the command line. This includes all the commands we introduced up until now, structures like for loops, output redirection, pipes, etc.\nTo run a bash script, you can simply execute the bash command and point it to a script. If we save the two lines above to script.sh file and then executing it by running bash script.sh, the single command inside of it will be executed and printed to the screen:\n$ bash script.sh\nMy first script!\nBelow is a slightly more complex example:\n#!/usr/bin/env bash\n\necho \"This is an example script.\"\n\necho \"The script was executed from the directory: \"\npwd\n\n# this is a comment\n\necho \"Running a for loop\"\n\nfor i in {1..5}\n        do echo i\ndone\n\necho \"This is a grep command\"\n\n# long commands can be split over multiple lines\n# this grep command counts the number of times \"tttataaaaaaac\"\n# occurs in the current directory and all of its subdirectories\n# while ignoring case\ngrep -i \\\n        -r \\\n        \"tttataaaaaaac\" \\\n        .\n\n\n\n\n\n\nTip\n\n\n\nNote that the indentation that we used is not strictly necessary for loops to work, but it does help with legibility and it is common practice to do this, especially in scripts.\nYou can use hashes (“#”) to comment out a line. Use this to describe what your code is doing. Your future you will be grateful!\nYou can also use backslash (“”) to split a long command over multiple lines, making it easier to read your script. E.g., for listing -options on consecutive lines.\n\n\nIf we save and run the script above, we get the following output.\nThis is an example script.\nThe script was executed from the directory:\n/home/pmoris/itg/FiMAB-bioinformatics/training/unix-demo\nThis is a for loop\ni\ni\ni\ni\ni\nThis is a grep command\n./PF0512_S47_L001_R1_001.fastq:CTAACTACAATGAAGACAAAAATATTATGTATATGTACCCAAATGAACCAAATTATAAGGATTCCAAAAAAGTATTATCTCAAAAAAAAAAAAAAAAAATCCACCATACATCATTTTCATCGTATTAATTCCCATGGACCACCTACACATGTGCAATTTATAAAAAAACAACAATCCCACTATCTCTAATACACATCTCCGAACCCACGAGACGCCGGACAATACCGTTTGCCAGCTCCCCGTACAATAAACCAATACTAAGATCATTGCCTCACTCTGAATCGCAGAACTCTGACGTATA\n./PF0512_S47_L001_R1_001.fastq:CTAACTACAATGAAGACAAAAATATTATGTATATGTACCCAAATGAACCAAATTATAAGGATTCCAAAAAAGTATTATCTCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAATTAAAAAAAAAAAATAAAAAAAAAAAAAATAAAAAAAAAAAAAAAAAAAAAAAAAATAAAAAAAAAAAAATAAAAAAAAAAAAAAAAAAAAAACAAAAACACAACAAAAACAAAAAATAAAATATATATTTATAAAAAAACAAAAAAAAGAAAACAACACAGACACTCAAACAACAACACACCA\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:TGAGATTGGATTTTTAAACATTAATATGGCGTGTTACATTTATAAAAAAACCCCAAAGAT\n./my_script.sh:# this grep command counts the number of times \"tttataaaaaaac\"\n./my_script.sh: \"tttataaaaaaac\" \\\nExiting script\n\n\n\n\n\n\nCan you spot what is wrong with this script? (Click me to expand!)\n\n\n\n\n\nThe output of the for loop section of the script is five lines of the character “i”, which is not what we wanted. The problem is that inside of the body of the for loop, we used the character “i”, instead of referencing the for loop variable using ${i}.\n\n\n\nLastly, note that not all scripts are bash scripts, you can also create R, Python or other types of scripts.\n\n\n8.3.3 Making scripts executable\nIn the previous examples we ran scripts by invoking them using the bash command. However, we can also make the file executable by using the chmod command. Once a file has been marked as executable for a given user, it can be run by simply typing the file name, without any other command in front.\n$ ls -l my_script.sh\n-rw-r--r-- 1 pmoris pmoris 488 Jan  4 14:35 my_script.sh\n\n$ chmod +x my_script.sh\n\n$ ls -l my_script.sh\n-rwxr-xr-x 1 pmoris pmoris 488 Jan  4 14:35 my_script.sh\n\n$ my_script.sh\n&lt;script output&gt;\nYou can read more about file permissions in Section A.5.\n\n\n\n\n\n\nDifferent methods of running script\n\n\n\nThere are actually a few different ways to execute a shell script:\n\nPrefixing the interpreter: bash path/to/script.sh. For this method, we explicitly execute the bash command and point it to the location of a script.\nDirectly running a script in the current working directory: ./script.sh. This option has two requirements: first, the file needs to be made executable (as we saw above). Furthermore, as a safety precaution, we need to prefix the name of the file with ./, to let the system know that we are trying to run a script that resides in the current working directory, as opposed to one that is globally accessible.\nDirectly running a globally accessible script: script.sh. This option also requires the file to be executable, but on top of that it will only work for files that are found in the list of directories making up your PATH (see Section A.4). These are pre-defined (or custom) directories like /usr/bin that usually store all the common commands that we have used until now, as well as any other software that you might install yourself."
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#overview-of-variant-calling-pipeline",
    "href": "content/unix/8-unix-variables-loops-scripts.html#overview-of-variant-calling-pipeline",
    "title": "8  Variables, loops and scripts",
    "section": "8.4 Overview of variant calling pipeline",
    "text": "8.4 Overview of variant calling pipeline\nThe chart below provides a high-level overview of the steps involved in a basic variant calling pipeline. Throughout the course, we have already introduced some of the file formats that are used (green). We will explore these steps and the associated tools, in more detail during the in-person courses later on.\n\n\n\nVariant calling pipeline\n\n\nFor now though, we have included a few example scripts that run through the above steps, to showcase how the building blocks that we have learned so far can be used to orchestrate a more complex analysis.\n\n\n\n\n\n\nNote\n\n\n\nInspect the scripts stored in ./training/scripts and try to make sense of the general steps that they describe. You can gloss over the specifics of what the specialized commands like fastqc or bwa mem do; instead, try to focus on the general structure and syntax of the scripts, the use of patterns like for loops, directory navigation, how arguments are provided to commands, etc.\n\n\n\n\n\n\n\n\nWhat does the following line do? sample_name=$(basename ${read_1} _R1_001.fastq.gz)  Hint: $(command) provided a method to run a command inside another statement, so try figuring out what the command between the brackets does first.\n\n\n\n\n\nThis line is present in most of the example scripts inside of a for loop that iterates over a set of FASTQ files, each time processing a pair of R1/R2 files.\n# first navigate to the directory containing fastq files\ncd ./training/data/fastq/\n\n# loop over pairs of fastq files\nfor read_1 in *_R1_001.fastq.gz\ndo\n    sample_name=$(basename ${read_1} _R1_001.fastq.gz)\n...\nDuring each iteration of the loop ${read_1} will correspond to the file path of a specific FASTQ file.\nWhen we call basename on it with the extra argument _R1_001.fastq.gz, we will receive back the name of the file with that suffix removed. E.g.:\n$ basename PF0080_S44_L001_R1_001.fastq.gz _R1_001.fastq.gz\nPF0080_S44_L001\nThe last step we do is running this command inside a command substitution: $(command). When using a command substitution, the output of the command inside the brackets will just be passed along to the command line. In our case, we try to assign the value $(basename ${read_1} _1.fastq.gz) to a new variable named sample_name. The value will then contain the output of its command substitution, namely PF0080_S44_L001.\nAfter running the above statement, we now are able to more easily construct the name of the first and second read pair:\n$ echo ${read_1} ${sample_name}_R2_001.fastq.gz\nPF0080_S44_L001_R1_001.fastq.gz PF0080_S44_L001_R2_001.fastq.gz\n\nThe first read we already had, but to create the second one we concatenate the sample name with the new suffix _R2_001.fastq.gz.\nPaired sequence data is usually named in such a way that it allows to access pairs of files in this way."
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#exercises",
    "href": "content/unix/8-unix-variables-loops-scripts.html#exercises",
    "title": "8  Variables, loops and scripts",
    "section": "8.5 Exercises",
    "text": "8.5 Exercises\n\nCreate a for loop over the files in ./training/unix-demo/files_to_loop_through that prints the first line of each file to the screen.\nCreate a bash script that does the same thing.\nOn a single-line, run the script, but sort its output in reverse order (check sort --help to check how) and store the new output in a file called loop_sort.txt.\nCreate a bash script with a for loop that prints the name of each read file in the ./training/data/fastq directory.\nModify the previous bash script so that it also 1) creates a single new directory named fastq_meta and 2) creates a new file in that directory, one for each FASTQ file, which contains two lines: the first with the number of lines in the FASTQ file and the second with its file size.\nCreate a bash script with a for loop that prints the sample name for each pair of reads in the ./training/data/fastq directory (i.e., half as many names as in the previous exercise).\nCreate a bash script that:\n\nCounts the number of header lines in ./training/unix-demo/ampliseq-variants.vcf and store this number as a variable.\nExtracts the contents of the VCF file after the header lines (i.e., the tabular section) and store it in a separate file.\nCreates a for loop to extract the first eight columns and store them each in a separate file named vcf_column_#.txt (where # is the column number)."
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#summary",
    "href": "content/unix/8-unix-variables-loops-scripts.html#summary",
    "title": "8  Variables, loops and scripts",
    "section": "8.6 Summary",
    "text": "8.6 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nVariables can be assigned via name=value and referenced via ${name}\nFor loops are used to iterate over a list of items or files\nScripts can be used to combine multiple commands into a single set of instructions that can be re-used.\nCommand substitution ($(command)), basename and for loops can be used to iterate over pairs of FASTQ files."
  },
  {
    "objectID": "content/r/r-intro.html",
    "href": "content/r/r-intro.html",
    "title": "R introduction",
    "section": "",
    "text": "As your first introduction to R, we will make use of Data Carpentry’s Intro to R and RStudio for Genomics course. This self-paced workshop walks through the basics of R - and its most popular IDE, RStudio - in the context of genomics. The focus lies on the basic syntax of R, wrangling tabular data using the tidyverse - where we will encounter the VCF file format once again (Section 7.1.1.1) - and visualization using ggplot2.\nThe Data Carpentry workshop will sometimes refer to running RStudio in a cloud environment with pre-installed packages and files, but when going through these materials on your own, we recommend installing R and RStudio on your own machine. You can do so by following the instructions listed here:https://rstudio-education.github.io/hopr/starting.html. You will also need to download the following two files, which are used throughout the workshop: combined_tidy_vcf.csv and Ecoli_metadata.xlsx.\nIf you do run into trouble installing R and RStudio on your own computer, you could instead:\n\nmake use of the free version of Posit Cloud (formerly known as RStudio Server): https://posit.cloud/plans/free. However, note that you only receive a limited number of hours of use as a free user.\nAlternatively, you can again make use of GitHub codespace, but you will need to select a different variant this time: RStudio Server.\nAs a final option, you could make use of a binder RStudio instance, such as this one."
  },
  {
    "objectID": "content/r/r-resources.html#cheatsheets",
    "href": "content/r/r-resources.html#cheatsheets",
    "title": "9  Other R resources",
    "section": "9.1 Cheatsheets",
    "text": "9.1 Cheatsheets\nEveryone loves cheatsheets!\n\nggplot2\ndplyr data transformations\ndata tidying using tidyr and tibble"
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-unix-tips",
    "href": "content/unix/appendix-unix.html#sec-unix-tips",
    "title": "Appendix A — Various Unix topics",
    "section": "A.1 Tips and hints",
    "text": "A.1 Tips and hints\n\n\n\n\n\n\nNaming conventions and cases\n\n\n\nNever (never!) use spaces in your file or directory names. This will only lead to pain… Instead, use hyphens (-) or underscores (_) to separate words. E.g., my_first_script and 3B207-2_S92_L001_R1_001.fastq.gz.\nAdditionally, unlike in Windows, in Unix everything is case-sensitive. Thus, /home/documents != /home/Documents. Be mindful of this when naming or pointing to files/directories.\n\n\n\n\n\n\n\n\nAutocompletion and command history\n\n\n\nAvoid unnecessary typing and just make things easy for yourself!\nWhile typing commands in the shell, you can almost always use the tab key for auto-completion. This will automatically type out paths, file names or known commands. If there exist multiple matches, a single press of tab will not appear to do anything, but if you press the button twice, a list of possible options will appear on your screen. This is incredibly useful, not only for speeding things up, but also for avoiding typos when dealing with long or complex file names.\nAn equally useful tool is your command history. While on the shell prompt, pressing the up arrow (↑) will bring up your most recent previous command. Pressing it again will cycle through the entire history, in reverse order. You can also search through your history by pressing ctrl+r allows you to search through your command history. Just start typing and you will see the search try to narrow down on the command that you are looking for. Once you find it, just press enter to run it directly or tab to copy it to your prompt (in case you still want to change it). The search form will look like this: (reverse-i-search)`world': echo \"Hello world!\"\n\n\n\n\n\n\n\n\nCopying and pasting\n\n\n\nCopying and pasting might work slightly different to what you are used to, depending on the terminal application that you are using. If ctrl+c and ctrl+v do not appear to work, you can tryctrl+shift+c and ctrl+shift+v instead. Often times, the mouse middle or right click can also be used for pasting.\nFor the native WSL terminal specifically, you can refer to this site for more info: https://devblogs.microsoft.com/commandline/copy-and-paste-arrives-for-linuxwsl-consoles/\n\n\n\n\n\n\n\n\nDon’t panic when you lose control of your shell!\n\n\n\nIf a command seems to hang or get stuck, your terminal becomes unresponsive, or if you tried to print a very large text file to your screen, you can use CTRL+C to interrupt almost any operation and regain control.\nSimilarly, CTRL+D is an often used shortcut for exiting/logging out (e.g., when dealing with remote servers of nested shells).\nIn some cases, like when using an interactive terminal program such as the text editors nano and vim or a text viewer like less, you will only be able to exit them using that particular program’s shortcut keys (CTRL+X, : followed by q and enter, and Q, for these applications respectively). For more info on terminal programs, check out Fantastic terminal programs and how to quit them.\n\n\n\n\n\n\n\n\nWatch out…\n\n\n\nBe careful while learning your way around the command-line. The Unix shell will do exactly what you tell it to, often without hesitation or asking for confirmation. This means that you might accidentally move, overwrite or delete files without intending to do so. For example, when creating, copying or moving files, they can overwrite existing ones if you give them the same name. Similarly, when a file is deleted, it will be removed completely, without first passing by a recycle bin.\nNo matter how much experience you have, it is a good idea to remain cautious when performing these types of operations.\nFor the purposes of learning, if you are using your own device instead of a cloud environment, we recommend that you work in a dedicated playground directory or even create a new user profile to be extra safe. And like always, backups of your important files are invaluable regardless of what you are doing.\n\n\n\n\n\n\n\n\nGoogle, -h/--help and comments are your friends.\n\n\n\nAt the beginning things will be awkward, so don’t worry about having to search for the same information multiple times. That is all part of the learning process. Moreover, being able to retrieve information when you are in need of a particular command is more useful than memorizing everything.\nIt can still be a good idea though to keep a list of commands that you often use, but have a difficult time committing to memory.\nMany commands will also display a short help text when called with the -h/--help flag. For some tools, you will need to call them without any arguments to display the help. Lastly, for some tools, you can use the man &lt;tool_name&gt; to open up an even more in-depth manual.\n\n\n\n\n\n\n\n\nMake your scripts easier to read by using comments and breaking up long lines\n\n\n\nRemember that you can always write comments inside of your scripts by starting a line with #. That way you can add a short explainer or extra info to the different sections of a script. Code that seems clear while you are writing it, has the unfortunate tendency of becoming much more confusing when you refer back to it at a later time.\n#!/usr/bin/env bash\n\n##########################################################\n# Script to map fastq files to reference genome with bwa #\n##########################################################\n\n# make sure to run this script from within the directory where it is stored!\n\n# move to the directory containing the fastq files\ncd ../data/fastq\n\n# create a directory to store the results and store the path as a variable\noutput_dir=\"../../results/bwa\"\nmkdir -p ${output_dir}\n\n...\nAdditionally, you can break up long commands using a \\ to make them easier to read. You can do this both in scripts or on the command line. E.g.,\nbwa mem \\\n  ../reference/PlasmoDB-65_Pfalciparum3D7_Genome.fasta \\\n  ${read_1} \\\n  ${sample_name}_R2_001.fastq.gz\n\n\n\n\n\n\n\n\nhttps://explainshell.com/ & https://tldr.inbrowser.app\n\n\n\nThe first website is tremendously useful for figuring out what a command and all of its options mean. Whereas the second shows you a quick summary of the most command usages of a particular command.\nUse both of these to your advantage! But do not forget that most commands also have a built-in help page that can be accessed using the --help flag (in some cases just typing the command without any arguments also shows some help information)."
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-table-special-syntax",
    "href": "content/unix/appendix-unix.html#sec-table-special-syntax",
    "title": "Appendix A — Various Unix topics",
    "section": "A.2 Overview of special syntax",
    "text": "A.2 Overview of special syntax\nThe table below gives you an overview of some of the special characters that we will encounter. You do not need to memorize them, but you can always refer back to this section if you see a symbol later on and are not quite sure what its purpose is.\n\n\n\n\n\n\n\n\nSymbol\nName\nUses\n\n\n\n\n/\nForward slash\nFile path separator or root location/file path\n\n\n\\\nBack slash\nSplit long command to a new line and escape special characters (+ file path separator in Windows)\n\n\n~\nTilde\nShortcut for home directory in file paths\n\n\n|\nPipe or vertical bar\nChains the output of one command to the input of another one (piping)\n\n\n#\nHash\nPart of the shebang at the top of scripts #! and used for comments in shell scripts\n\n\n$\nDollar sign\nUsed to access variables in bash\n\n\n*\nAsterisk or wildcard\nGlobbing operator\n\n\n&gt;\nGreater than symbol\nRedirect output of a command (&gt;&gt; redirect and append instead of overwriting)\n\n\n&lt;\nLess than symbol\nRedirect input to a command\n\n\n.\nDot\nIn the context of a path, it represents the current working directory\n\n\n..\nDouble dot\nIn the context of a path, it represents the parent directory of the working directory"
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-unix-sudo",
    "href": "content/unix/appendix-unix.html#sec-unix-sudo",
    "title": "Appendix A — Various Unix topics",
    "section": "A.3 Understanding superusers, root and sudo",
    "text": "A.3 Understanding superusers, root and sudo\nSuperusers are special users on Unix systems with additional privileges (cf. Windows administrator). By convention, the default name for a superuser on Linux is root (don’t confuse this with the root of the filesystem /). Superusers have access to all files and directories on the file system, including any critical components that make the system work and any files owned by other users. Moreover, several tasks like software installation and modifying system configuration require administrative privileges. It would be a security risk to run as a superuser constantly, but how can regular users install software then? Or what about modifying the permissions of a file that you (accidentally) removed your own access to? This is where the sudo command comes into play.\nsudo stands for “superuser do” and it allows regular users, who have been added to the sudo user group, to run individual commands as if they were the root user. A common use-case is using sudo to install new software via apt (on Debian-like systems) or dnf (on Fedora/CentOS): sudo apt install ncbi-blast+.\nWith great power comes great responsibility, so exercise caution and think twice before running a command with sudo. Only do so when you are sure you know what the end result will be."
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-unix-path",
    "href": "content/unix/appendix-unix.html#sec-unix-path",
    "title": "Appendix A — Various Unix topics",
    "section": "A.4 What is $PATH?",
    "text": "A.4 What is $PATH?\nThe $PATH is a way of letting your computer know where specific tools or other special locations are stored on your file system. Unless you tell it explicitly, it won’t know where to find any new software you install. Fortunately, most methods of installing software automatically take care of this for you, but every now and then you will need to manually add things to your $PATH. If you don’t, you will be greeted by messages like Command 'python' not found, did you mean:.\nThe $PATH is nothing more than a list of locations on your computer. Everything that is found in those locations, will become available to use directly on the CLI without having to type out its full location. Even the basic Unix commands, like ls and cd are only known to your shell because they are in a location that is indexed by your path.\nIn the following example, we will demonstrate how you can add a custom directory with scripts to your $PATH, making them callable from anywhere.\n# show the contents of PATH\necho $PATH\n/home/pmoris/miniforge3/bin:/home/pmoris/miniforge3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/windows/system32:/mnt/c/windows:/mnt/c/windows/System32/Wbem:/mnt/c/windows/System32/WindowsPowerShell/v1.0:/mnt/c/windows/System32/OpenSSH:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/dotnet:/mnt/c/Users/pmoris/AppData/Local/Programs/Quarto/bin:/mnt/c/Users/pmoris/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/pmoris/AppData/Local/Programs/Microsoft VS Code/bin:/snap/bin\n\n# temporarily add directory of scripts to PATH\n$ ls ~/itg/FiMAB-bioinformatics/training/scripts\ncall_variants.sh  download_reference.sh  map.sh  remove_dups.sh  trim.sh\n$ export PATH=\"$PATH:~/itg/FiMAB-bioinformatics/training/scripts\"\n\n# now these scripts can be invoked directly without having to type out their full location\n# e.g., map.sh works just as well as ~/itg/FiMAB-bioinformatics/training/scripts/map.sh\nTo make these changes permanent, you’d have to add that export statement to your .bashrc file (stored in your home directory). This file is run every time you launch a new shell, so that will allow the $PATH to be modified every time during startup.\nYou can find more information on modifying the PATH here.\nLastly, be careful when modifying your PATH. If you mess it up, it can cause all kinds of havoc."
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-permissions",
    "href": "content/unix/appendix-unix.html#sec-permissions",
    "title": "Appendix A — Various Unix topics",
    "section": "A.5 Dealing with file permissions",
    "text": "A.5 Dealing with file permissions\nYou can find an excellent explanation on file permissions here."
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-ssh",
    "href": "content/unix/appendix-unix.html#sec-ssh",
    "title": "Appendix A — Various Unix topics",
    "section": "A.6 Working with remote machines via SSH",
    "text": "A.6 Working with remote machines via SSH\nIn some cases, you will need to work on a Linux machine that is physically located somewhere else, i.e. a remote server. Access to these is usually managed via a command-line tool called SSH (or a stand-alone GUI tool like Putty in Windows). The syntax of the ssh command is as follows:\nssh username@domain\nWhere username is a name given to you by the admin of the system and domain is the address of the server (can be a URL or an IP address).\nThe connection is secured via SSH keys: a pair of files used for authentication stored in ~/.ssh.\n\nPublic key: e.g., id_rsa.pub or id_ed25519.pub, located on the remote server.\nPrivate file: e.g., id_rsa or id_ed25519.pub, located on your own machine. *Never share this file with anyone else!**\n\nInstructions to generate new SSH keys can be found https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent.\nA common problem when connecting is that the file permissions of your keys or credential files are messed up. This can happen if you generate them in Windows and later move them to a Linux file system. To fix this, check https://superuser.com/a/215506.\nMore info on remote servers can be found here.\nLastly, keep in mind that when working on remote servers, it is essential to use screen or tmux for long-running jobs. Otherwise, they will be interrupted when you disconnect or even when the network briefly fails."
  },
  {
    "objectID": "content/unix/appendix-unix.html#further-reading",
    "href": "content/unix/appendix-unix.html#further-reading",
    "title": "Appendix A — Various Unix topics",
    "section": "A.7 Further reading",
    "text": "A.7 Further reading\nThere are still many facets of Unix that we have not covered here yet, but we hope that you can pick these up on your own if were to ever need them. For now though, the basics that we covered here, will hopefully already go a long way in helping you use Unix and read scripts written by others.\nA selection of more advanced topics to explore at a later time could be:\n\nInstalling software on Linux machines, from source or via apt (Ubuntu), dnf (Fedora) or pacman (Arch), or through tools like conda/Miniforge and bioconda.\nThe find command: useful for finding files in your file system.\nIf/else conditions: allow you to execute parts of scripts if certain conditions are fulfilled.\nCommand substitution and process substitution: run commands in a subshell to allow more complex ways of redirecting and piping outputs/inputs.\nsed/awk are commands that let you do things like search and replace, calculations or other manipulations based on all the lines of a file.\nthe join command: combining columns of (multiple) tabular data files in particular ways.\nEnvironment variables: variables that are always available, like PATH.\nUsing screen or tmux to spawn persistent background terminal sessions. This allows you to run commands on a remote server and shutdown your own machine, without the remote process being interrupted.\nLearn about the difference in line endings on Unix (\\n) and Windows (\\r\\n), how to switch between them (dos2unix) and how to set a preference in your editors like RStudio.\nLearn about sudo, allowing you to perform actions as an administrator.\nRegular expressions, to power up your grep searches and sed/awk commands."
  }
]